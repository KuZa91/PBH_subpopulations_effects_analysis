{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> SGWB Parameter Space Analysys</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we'll implement a notebook that, given the required Probability Distribution Functions (PDF) describing a Black Hole population(BH), generates the figure of merit for the predicted analytical Stochastic Gravitational Wave Background(SGWB) in function of the amplitude and redshift range of the merging rate.\n",
    "First of all, we need to import some modules ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special as sc\n",
    "import statistics as st\n",
    "import random\n",
    "import os\n",
    "import IPython\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "import scipy.stats as scst\n",
    "#from tqdm import tqdm\n",
    "from scipy import interpolate\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import quad, simpson, dblquad\n",
    "from scipy.stats import poisson\n",
    "from scipy.special import gamma, hyp1f1\n",
    "from scipy.optimize import minimize\n",
    "from pycbc import waveform as wf\n",
    "from multiprocessing import Pool, Manager, Process, Value, Array\n",
    "from functools import partial\n",
    "#from LISAhdf5 import LISAhdf5,ParsUnits\n",
    "#%matplotlib inline\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.style.use(\"seaborn-v0_8-whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Global Variables of the Simulation </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The global variables of the simulation will be set to :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flags for the execution modes, initialized to false, check the the FLAG selection section for additional informations and initializing them !\n",
    "\n",
    "PBH_LogNormal = False\n",
    "PBH_Gaussian = False\n",
    "\n",
    "# Merger distribution parameters\n",
    "\n",
    "T_obs = 10. # Lisa or LIGO estimated years of observation\n",
    "efficiency = 1. # Lisa effective usefull time percentage for observations\n",
    "max_tc = 10000. # max years of coalescence time for a BBH mergine event\n",
    "frq_min = 3.e-5 # Hertz\n",
    "frq_max = 0.5 # Maximum frequency in hertz to which the LISA detector is sensitive\n",
    "frq_star = 1.e-2 # Value of the choosen frequency at which we estimate the SGWB to compare with other results\n",
    "# The total time used to generate the merging events by multipling for the rate of merging will be set to max_tc\n",
    "\n",
    "\n",
    "#General Constants \n",
    "\n",
    "c = 299792.46 # speed of light in Km/sec\n",
    "G = 6.674*(10.**(-11.)) # Gravitational constant in m^3⋅kg^−1⋅s^−2\n",
    "sol_mass = 1.988e30 # Value of the Solar Mass in Kg\n",
    "MPc = 3.08567758149137*1e22 # meters\n",
    "GPc = MPc*1e3 # meters\n",
    "sig_M = 0.004 # rescaled variance of matter density perturbations\n",
    "h = 0.678\n",
    "H_0 = 67.8*1e3/MPc # Hubble constant in 1/(s)\n",
    "Omega_m = 0.3 # Matter density in our universe\n",
    "Omega_lambda = 0.7 # Cosmological constant density in our universe\n",
    "Omega_k = 0. # Curvature density in our universe\n",
    "rho_c = (3.*(H_0**2.))/(8.*np.pi*G) # Critical density in our universe\n",
    "year = 365.25*24*60*60 # Years in second \n",
    "    \n",
    "# Precision settings for the binned variables\n",
    "\n",
    "n_jobs = 80\n",
    "frq_res = 1e-6\n",
    "frq_prec = int((frq_max - frq_min)/frq_res) + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> FLAG selection section </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to choose between the two following different mass functions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PBH_LogNormal = True# This will simulate the Log Normal mass distribution for PBH described in ariv 2109.05836\n",
    "PBH_Gaussian = True # This will simulate a Gaussian PBH mass distribution, that can be used to generalize a bit the standard monocromatic mass function for PBH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Utility functions </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we are going to define some useful generical functions that will be needed to present the results.\n",
    "We will start with a function that can be used to convert matplotlib contour line to arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contour_verts(cn):\n",
    "    # Given a set of contour line, save them as a dictionary\n",
    "    contours = []\n",
    "    # for each contour line\n",
    "    for cc in cn.collections:\n",
    "        paths = []\n",
    "        # for each separate section of the contour line\n",
    "        for pp in cc.get_paths():\n",
    "            xy = []\n",
    "            # for each segment of that section\n",
    "            for vv in pp.iter_segments():\n",
    "                xy.append(vv[0])\n",
    "            paths.append(np.vstack(xy))\n",
    "        contours.append(paths)\n",
    "\n",
    "    return contours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Standard Cosmological Functions </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we'll need a function that allow us to convert from redshift to Gigaparsec :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a function to convert from Z to GPC using Hubble Law, in order to obtain the comoving distance\n",
    "\n",
    "z_max = 1.e8\n",
    "z_prec = 10000\n",
    "\n",
    "def H(z):\n",
    "    return np.sqrt((H_0**2.)*(Omega_m*((1. + z)**3.) + Omega_k*((1. + z)**2.) + Omega_lambda))\n",
    "\n",
    "def Z_to_Gpc(z):\n",
    "    \n",
    "    # Remove the commented part to use a linear approximation of the Hubble law for low z \n",
    "    \n",
    "    #if(zmax <= 0.5):\n",
    "    #    return ((z*c*(10**(-3)))/(H_0)) # only valid for z < 0.5\n",
    "    #else:\n",
    "        \n",
    "        span_z = np.linspace(0.,z,z_prec)\n",
    "        span_mz = 0.5*(span_z[1::] + span_z[:-1:])\n",
    "        \n",
    "        # Beware, would fail if the span z is created in logarithmic scale !\n",
    "        \n",
    "        Int_Z = c*(10**(-3))*simpson(1./(H(span_mz)*(MPc/1.e3)), span_mz, axis=0)\n",
    "    \n",
    "        return Int_Z\n",
    "    \n",
    "def Z_to_HubbleTime(z):\n",
    "    \n",
    "    span_z = np.logspace(np.log10(z),np.log10(z_max),z_prec)\n",
    "    span_mz = 0.5*(span_z[1::] + span_z[:-1:])\n",
    "        \n",
    "    # Beware, would fail if the span z is created in logarithmic scale !\n",
    "        \n",
    "    \n",
    "    Int_Z = simpson(1./(H(span_mz)*(1. + span_mz)), span_mz, axis=0)\n",
    "    \n",
    "    return Int_Z\n",
    "    \n",
    "t_0 = Z_to_HubbleTime(1.e-12) # Can't put 0 as the logarithmic scale would fail        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we also need a function that estimates the differential comoving volume in function of the redshift :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the following function, the differential comoving volume in function of the redshift will be estimated as a spherical surface, it need to be integrated over dr to obtain the real volume \n",
    "\n",
    "def DeVC(z, Delta_z):\n",
    "    r = dist_func(z)\n",
    "    z_2 = z + 0.5*Delta_z\n",
    "    z_1 = z_2 - Delta_z\n",
    "    Delta_r = dist_func(z_2) - dist_func(z_1)\n",
    "    return ((4.*np.pi*(r**2.)*Delta_r)/Delta_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another recurring parameter for inspiralling events is the Chirp Mass and Reduced Mass, given the mass of the two events involved in the binary merging :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that return the Chirp Mass of a binary merging event\n",
    "\n",
    "def ChirpMass(m1,m2): \n",
    "   return ((m1*m2)**(3./5.))/((m1+m2)**(1./5.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EtaMass(m1,m2):\n",
    "    return (m1*m2)/((m1 + m2)**2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def S1(f_PBH, m1, m2, avg_m, avg_m2):\n",
    "    res = 1.42*((avg_m2/(avg_m**2.))/(N_MinDist(f_PBH, m1, m2, avg_m) + C_S1(f_PBH, avg_m, avg_m2)) + (sig_M**2.)/(f_PBH**2.))**(-21./74.) \n",
    "    return (res*np.exp(-N_MinDist(f_PBH, m1, m2, avg_m)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_MinDist(f_PBH, m1, m2, avg_m):\n",
    "    # Return the minimal distance of the third PBH required for smoothing\n",
    "    res = ((m1 + m2)/avg_m)*(f_PBH/(f_PBH + sig_M))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def C_S1(f_PBH, avg_m, avg_m2):\n",
    "    res = ((f_PBH**2.)*(avg_m2/(avg_m + sig_M)**2.))/( ((gamma(29./37.)/np.sqrt(np.pi))*hyp1f1(21./74., 0.5, (5.*f_PBH**2.)/(6.*sig_M**2.)))**(-74./21.) - 1.)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> SOBBH - Redshift dependent statistic </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may now define, the various implemented merging rates as a function of the redshift _z_ as :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for the merging rate as described in the paper arxiv 2010.14533, the flag Red_evol will decide if adopting a merging rate the evolve with redshift (true) or not (false)\n",
    "\n",
    "\n",
    "SOBBH_k = 2.9 # + 1.7 - 1.8  VALID FOR REDSHIFT EVOLVING POWER LAW + PEAK MODEL MASS DISTRIBUTION, total agreement with SFR\n",
    "SOBBH_CorrRz = (((1. + 0.2)**SOBBH_k)/(1. + ((1. + 0.2)/2.9)**(SOBBH_k + 2.9)))**(-1) # Normalization factor estimated at z = 0.2\n",
    "    \n",
    "# Defining the value of R0, the 0 index will have the value for redshift evolution merging rate, the 1 index would have the one for constant merging rate\n",
    "\n",
    "\n",
    "SOBBH_R0= 28.3# +13.9 - 9.1 GPC⁻³ yr^⁻¹ Value of the merging rate fitted at z = 0.2\n",
    "\n",
    "\n",
    "def SOBBH_R(z):\n",
    "    return SOBBH_R0[0]*SOBBH_CorrRz*((1. + z)**SOBBH_k)/(1. + ((1. + z)/2.9)**(SOBBH_k + 2.9))\n",
    "    \n",
    "# If we wish to generate just a spike of events at a certain redshift range coming from a merging rate with fixed amplitude, we fix the following        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> PBH - Merging Rates </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same model described by [S. S. Bavera et al](https://arxiv.org/pdf/2109.05836.pdf) for the redshift evolution of the merging rate. The amplitude of the perturbation can still be parametrized using the $fR$ approach as in the previous model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PBH_R0 = 28.3 # +14.8 - 10.0 GPC⁻³ yr^⁻¹ Value of the merging rate fitted in at z = 0.2 in ligo population inference paper arxiv2111.03634\n",
    "PBH_CorrfRz = SOBBH_CorrRz  # normalization factor needed to express the value of the LIGO merging rate in z=0\n",
    "    \n",
    "def PBH_fR(z,f):\n",
    "    return f*PBH_R0*PBH_CorrfRz*((t_z(z)/t_0)**(-34./37.))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> PBH - Gaussian Mass Distribution </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a Gaussian mass distribution for PBH as :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PBH_Gaussian:\n",
    "    PBH_m = 0. # Solar Masses. Minimum value assumed for the PBH mass\n",
    "    PBH_M = 150. # Solar Masses. Maximum value assumed for the PBH mass\n",
    "    PBH_massprec = 300 # Binning density for the mass range\n",
    "    PBH_pdfmspan = np.linspace(0., 100., 100) # this span will be needed to compute the figures of merit\n",
    "    PBH_sigmamspan = [1. ,5. ,10. ,15.] # Values of sigma_m to be spanned by the simulation\n",
    "    \n",
    "    # We use the following distribution for the mass, this tend to a monochromatic mass function for small values of sigma, yet it can be used to generalize the result to a wider subset of cases\n",
    "    def PBH_MassGauss(m, PBH_mu, PBH_sigmam, PBH_GSnorm):\n",
    "        return ((1./(PBH_sigmam*np.sqrt(2.*np.pi)))*np.exp(-0.5*((m-PBH_mu)/PBH_sigmam)**2.))*1./PBH_GSnorm\n",
    "    \n",
    "    # This function is to estimate the normalization constant\n",
    "    def PBH_GaussPS(PBH_ranm, PBH_mu, PBH_sigmam):\n",
    "\n",
    "        PBH_midm = 0.5*(PBH_ranm[1::] + PBH_ranm[:-1:])\n",
    "\n",
    "        ris =  simpson(PBH_MassGauss(PBH_midm, PBH_mu, PBH_sigmam, 1.), PBH_midm)\n",
    "            \n",
    "        return ris\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> PBH - Log-Normal Mass Distribution </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a Log-Normal mass distribution for PBH as described in the paper by [S. S. Bavera et al ](https://arxiv.org/abs/2109.05836):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PBH_LogNormal:\n",
    "    # We use the following distribution for the mass\n",
    "    PBH_m = 0. # Solar Masses. Minimum value assumed for the PBH mass\n",
    "    PBH_M = 150. # Solar Masses. Maximum value assumed for the PBH mass\n",
    "    PBH_massprec = 300 # Binning density for the mass range\n",
    "    PBH_pdfmspan = np.linspace(0, 100., 100) # this span will be needed to compute the figures of merit\n",
    "   \n",
    "    PBH_sigmamnspan = [0.1 ,0.5 ,1. ,2.5] # Values of sigma_m to be spanned by the simulation\n",
    "    \n",
    "    def PBH_MassLNorm(m, PBH_Mc, PBH_sigmamn, PBH_LNnorm):\n",
    "        return (1./(np.sqrt(2*np.pi)*PBH_sigmamn*m))*np.exp(-(np.log(m/PBH_Mc)**2)/(2*PBH_sigmamn**2))*1./PBH_LNnorm\n",
    "    \n",
    "    # This function is to estimate the normalization constant\n",
    "    def PBH_LNnormPS(PBH_ranm, PBH_Mc, PBH_sigmamn):\n",
    "        \n",
    "        PBH_midm = 0.5*(PBH_ranm[1::] + PBH_ranm[:-1:])\n",
    "\n",
    "        ris =  simpson(PBH_MassLNorm(PBH_midm, PBH_Mc, PBH_sigmamn, 1.), PBH_midm)\n",
    "            \n",
    "        return ris\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Functions to estimate the $f_{PBH}$ given the value of $\\varepsilon$</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to minimize in order to find the value of $f_{PBH}$ given $\\varepsilon$ is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_resid(f_PBH, our_f, PBH_mu, PBH_sigma, PBH_norm, Avg_m, Avg_msqrd):\n",
    "    if PBH_LogNormal:\n",
    "        to_int = lambda y, x : ((x + y)**(-32./37.))*(EtaMass(x,y)**(-34./37.))*S1(f_PBH, x, y, Avg_m, Avg_msqrd)*PBH_MassLNorm(x, PBH_mu, PBH_sigma, PBH_norm)*PBH_MassLNorm(y, PBH_mu, PBH_sigma, PBH_norm)\n",
    "    if PBH_Gaussian:\n",
    "        to_int = lambda y, x : ((x + y)**(-32./37.))*(EtaMass(x,y)**(-34./37.))*S1(f_PBH, x, y, Avg_m, Avg_msqrd)*PBH_MassGauss(x, PBH_mu, PBH_sigma, PBH_norm)*PBH_MassGauss(y, PBH_mu, PBH_sigma, PBH_norm)\n",
    "    return ((1.6e6/(PBH_R0*PBH_CorrfRz))*(f_PBH**(53./37.))*dblquad(to_int, PBH_midm[0], PBH_midm[-1], PBH_midm[0], PBH_midm[-1], epsrel=1.e-4)[0] - our_f)**2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate this results over the maps by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FpbhToVarEpsilon(i, mat):        \n",
    "        \n",
    "    # Estimating the variables for the Gaussian Case \n",
    "        \n",
    "    if PBH_Gaussian:\n",
    "        PBH_mu = 0.5*(PBH_pdfmspan[i + 1] + PBH_pdfmspan[i])\n",
    "        PBH_GSnorm = PBH_GaussPS(PBH_midm, PBH_mu, PBH_sigmam)\n",
    "        Avg_m = simpson(PBH_midm*PBH_MassGauss(PBH_midm, PBH_mu, PBH_sigmam, PBH_GSnorm), PBH_midm)\n",
    "        Avg_msqrd = simpson(PBH_midm*PBH_midm*PBH_MassGauss(PBH_midm, PBH_mu, PBH_sigmam, PBH_GSnorm), PBH_midm)\n",
    "        \n",
    "    # Estimating the variables for the LogNormal case\n",
    "        \n",
    "    if PBH_LogNormal:\n",
    "        PBH_mu = 0.5*(PBH_pdfmspan[i + 1] + PBH_pdfmspan[i])\n",
    "        PBH_LNnorm = PBH_LNnormPS(PBH_midm, PBH_mu, PBH_sigmamn)\n",
    "        Avg_m = simpson(PBH_midm*PBH_MassLNorm(PBH_midm, PBH_mu, PBH_sigmamn, PBH_LNnorm), PBH_midm)\n",
    "        Avg_msqrd = simpson(PBH_midm*PBH_midm*PBH_MassLNorm(PBH_midm, PBH_mu, PBH_sigmamn, PBH_LNnorm), PBH_midm)\n",
    "       \n",
    "    # Defining common variables\n",
    "    \n",
    "    frange_inv = PBH_frange[::-1]\n",
    "    res_inv = frange_inv*0.\n",
    "    Initial_guess = 0.005\n",
    "    \n",
    "    # Running the minimization\n",
    "    \n",
    "    for j in range(len(frange_inv)):\n",
    "        our_f = frange_inv[j]\n",
    "        if PBH_Gaussian:\n",
    "            to_minimize = partial(f_resid, our_f = our_f, PBH_mu = PBH_mu, PBH_sigma = PBH_sigmam, PBH_norm = PBH_GSnorm, Avg_m = Avg_m, Avg_msqrd = Avg_msqrd)\n",
    "        if PBH_LogNormal:\n",
    "            to_minimize = partial(f_resid, our_f = our_f, PBH_mu = PBH_mu, PBH_sigma = PBH_sigmamn, PBH_norm = PBH_LNnorm, Avg_m = Avg_m, Avg_msqrd = Avg_msqrd)\n",
    "        if j == 0:\n",
    "            res_inv[j] = minimize(to_minimize, Initial_guess, method='Nelder-Mead', options={'xatol':1.e-5, 'fatol':1.e-5}).x\n",
    "        else:\n",
    "            res_inv[j] = minimize(to_minimize, res_inv[j-1], method='Nelder-Mead', options={'xatol':1.e-5, 'fatol':1.e-5}).x\n",
    "     \n",
    "    res = res_inv[::-1]\n",
    "    \n",
    "    # Saving the dataset\n",
    "    \n",
    "    for j in range(len(frange_inv)):\n",
    "        delta_val = pd.DataFrame([[i, j, res[j]],], columns = ['idx_pdfm', 'idx_var_epsilon', 'val'])\n",
    "        mat.append(delta_val)                       \n",
    "        \n",
    "    if ((i*10)%len(PBH_pdfmspan) == 0) :\n",
    "        print('Percentage of completition : ',(i*100.)/(len(PBH_pdfmspan)), '%', flush=True)\n",
    "        \n",
    "                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Setting of the analyzed phase space </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulation will be spanned over the following range of variables :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the mas range\n",
    "PBH_ranm1 = np.linspace(PBH_m, PBH_M, PBH_massprec)\n",
    "PBH_ranm2 = PBH_ranm1\n",
    "PBH_midm = 0.5*(PBH_ranm1[1::] + PBH_ranm1[:-1:])\n",
    "PBH_frange = np.logspace(-3.,0.,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Main body of the simulation </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the conversion map for the considered _PBH_ subpopulation, we can now run as a function of the _Mass PDF_ parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~\n",
      "-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~\n",
      "Now simulating the Integrated factor for PBH (part  1  of  4 ), this can take some time !\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PBH_GSnorm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/kuza91/anaconda3/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/kuza91/anaconda3/lib/python3.8/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"/tmp/ipykernel_25432/2358354409.py\", line 34, in FpbhToVarEpsilon\n    res_inv[j] = minimize(to_minimize, Initial_guess, method='Nelder-Mead', options={'xatol':1.e-5, 'fatol':1.e-5}).x\n  File \"/home/kuza91/anaconda3/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 611, in minimize\n    return _minimize_neldermead(fun, x0, args, callback, bounds=bounds,\n  File \"/home/kuza91/anaconda3/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 750, in _minimize_neldermead\n    fsim[k] = func(sim[k])\n  File \"/home/kuza91/anaconda3/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 464, in function_wrapper\n    return function(np.copy(x), *(wrapper_args + args))\n  File \"/tmp/ipykernel_25432/2357065981.py\", line 6, in f_resid\n    return ((1.6e6/(PBH_R0*PBH_CorrfRz))*(f_PBH**(53./37.))*dblquad(to_int, PBH_midm[0], PBH_midm[-1], PBH_midm[0], PBH_midm[-1], epsrel=1.e-4)[0] - our_f)**2.\n  File \"/home/kuza91/anaconda3/lib/python3.8/site-packages/scipy/integrate/quadpack.py\", line 601, in dblquad\n    return nquad(func, [temp_ranges, [a, b]], args=args,\n  File \"/home/kuza91/anaconda3/lib/python3.8/site-packages/scipy/integrate/quadpack.py\", line 825, in nquad\n    return _NQuad(func, ranges, opts, full_output).integrate(*args)\n  File \"/home/kuza91/anaconda3/lib/python3.8/site-packages/scipy/integrate/quadpack.py\", line 879, in integrate\n    quad_r = quad(f, low, high, args=args, full_output=self.full_output,\n  File \"/home/kuza91/anaconda3/lib/python3.8/site-packages/scipy/integrate/quadpack.py\", line 351, in quad\n    retval = _quad(func, a, b, args, full_output, epsabs, epsrel, limit,\n  File \"/home/kuza91/anaconda3/lib/python3.8/site-packages/scipy/integrate/quadpack.py\", line 463, in _quad\n    return _quadpack._qagse(func,a,b,args,full_output,epsabs,epsrel,limit)\n  File \"/home/kuza91/anaconda3/lib/python3.8/site-packages/scipy/integrate/quadpack.py\", line 879, in integrate\n    quad_r = quad(f, low, high, args=args, full_output=self.full_output,\n  File \"/home/kuza91/anaconda3/lib/python3.8/site-packages/scipy/integrate/quadpack.py\", line 351, in quad\n    retval = _quad(func, a, b, args, full_output, epsabs, epsrel, limit,\n  File \"/home/kuza91/anaconda3/lib/python3.8/site-packages/scipy/integrate/quadpack.py\", line 463, in _quad\n    return _quadpack._qagse(func,a,b,args,full_output,epsabs,epsrel,limit)\n  File \"/tmp/ipykernel_25432/2357065981.py\", line 5, in <lambda>\n    to_int = lambda y, x : ((x + y)**(-32./37.))*(EtaMass(x,y)**(-34./37.))*S1(f_PBH, x, y, Avg_m, Avg_msqrd)*PBH_MassGauss(x, PBH_mu, PBH_sigmam, PBH_GSnorm)*PBH_MassGauss(y, PBH_mu, PBH_sigmam, PBH_GSnorm)\nNameError: name 'PBH_GSnorm' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:                                    \n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# start the worker processes equals to n_jobs\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     pool \u001b[38;5;241m=\u001b[39m Pool(n_jobs)\n\u001b[0;32m---> 49\u001b[0m     \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFpbhToVarEpsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43md_par\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPBH_pdfmspan\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     pool\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     51\u001b[0m     pool\u001b[38;5;241m.\u001b[39mjoin()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/multiprocessing/pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/multiprocessing/pool.py:771\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PBH_GSnorm' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if PBH_LogNormal:\n",
    "    \n",
    "    #Summing on the PBH background contribution in the case of a LogNormal PDF\n",
    "    print('-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~')\n",
    "    \n",
    "    manager = Manager()\n",
    "    ConvMap = {}\n",
    "    \n",
    "    for i in range(len(PBH_sigmamnspan)):\n",
    "        print('-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~')\n",
    "        Conv_app = np.zeros((len(PBH_pdfmspan) - 1, len(PBH_frange))) + 10.\n",
    "        PBH_sigmamn = PBH_sigmamnspan[i]\n",
    "        d_par = manager.list()\n",
    "        print('Now simulating the Conversion map for PBH (part ',i + 1,' of ',len(PBH_sigmamnspan),'), this can take some time !')\n",
    "        \n",
    "        if __name__ == '__main__':                                    \n",
    "            # start the worker processes equals to n_jobs\n",
    "            pool = Pool(n_jobs)\n",
    "            pool.map(partial(FpbhToVarEpsilon, mat = d_par), range(len(PBH_pdfmspan)-1))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "        \n",
    "        for count in range(len(d_par)):\n",
    "            Conv_app[int(d_par[count]['idx_pdfm'])][int(d_par[count]['idx_var_epsilon'])] = float(d_par[count]['val'])\n",
    "           \n",
    "        \n",
    "        ConvMap[i] = Conv_app.transpose()\n",
    "        print('-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~')\n",
    "\n",
    "if PBH_Gaussian:\n",
    "    \n",
    "    #Summing on the PBH background contribution for the case of a Gaussian PDF with several sigma_m\n",
    "    print('-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~')\n",
    "    \n",
    "    manager = Manager()\n",
    "    ConvMap = {}\n",
    "    \n",
    "    for i in range(len(PBH_sigmamspan)):\n",
    "        print('-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~')\n",
    "        Conv_app = np.zeros((len(PBH_pdfmspan) - 1, len(PBH_frange))) + 10.\n",
    "        PBH_sigmam = PBH_sigmamspan[i]\n",
    "        d_par = manager.list()\n",
    "        print('Now simulating the Integrated factor for PBH (part ',i + 1,' of ',len(PBH_sigmamspan),'), this can take some time !')\n",
    "                   \n",
    "        if __name__ == '__main__':                                    \n",
    "            # start the worker processes equals to n_jobs\n",
    "            pool = Pool(n_jobs)\n",
    "            pool.map(partial(FpbhToVarEpsilon, mat = d_par), range(len(PBH_pdfmspan)-1))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "        \n",
    "        for count in range(len(d_par)):\n",
    "            Conv_app[int(d_par[count]['idx_pdfm'])][int(d_par[count]['idx_var_epsilon'])] = float(d_par[count]['val'])\n",
    "           \n",
    "        \n",
    "        ConvMap[i] = Conv_app.transpose()\n",
    "        print('-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving the dataset </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the data using :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the values of z at which the perturbation will overtake the fiducial models\n",
    "\n",
    "if PBH_LogNormal:\n",
    "    fname = 'ConvMapLNPDF.pickle'\n",
    "if PBH_Gaussian:\n",
    "    fname = 'ConvMapGSPDF.pickle'\n",
    "    \n",
    "file_to_write = open(fname, \"wb\")\n",
    "pickle.dump(ConvMap, file_to_write)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Setting alarm to inform when simulation is over </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = 'Alarm-ringtone.mp3'\n",
    "#os.system(\"mpg123 \"+file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
