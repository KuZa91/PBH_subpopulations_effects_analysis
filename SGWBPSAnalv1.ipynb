{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> SGWB Parameter Space Analysys</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we'll implement a notebook that, given the required Probability Distribution Functions (PDF) describing a Black Hole population(BH), generates the figure of merit for the predicted analytical Stochastic Gravitational Wave Background(SGWB) in function of the amplitude and redshift range of the merging rate.\n",
    "First of all, we need to import some modules ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special as sc\n",
    "import statistics as st\n",
    "import random\n",
    "import os\n",
    "import IPython\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "import scipy.stats as scst\n",
    "#from tqdm import tqdm\n",
    "from scipy import interpolate\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import quad, simpson\n",
    "from scipy.stats import poisson\n",
    "from scipy.special import gamma, hyp1f1\n",
    "from pycbc import waveform as wf\n",
    "from multiprocessing import Pool, Manager, Process, Value, Array\n",
    "from functools import partial\n",
    "#from LISAhdf5 import LISAhdf5,ParsUnits\n",
    "#%matplotlib inline\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.style.use(\"seaborn-v0_8-whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Global Variables of the Simulation </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The global variables of the simulation will be set to :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flags for the execution modes, initialized to false, check the the FLAG selection section for additional informations and initializing them !\n",
    "\n",
    "# Flag needed to simulate the standard LIGO SOBBH population\n",
    "\n",
    "SOBBH = False\n",
    "SOBBH_Redevol = False\n",
    "SOBBH_RSpike = False\n",
    "\n",
    "# Flags for different types of PBH mass distribution\n",
    "\n",
    "PBH = False\n",
    "PBH_fRz = False\n",
    "PBH_fRt = False\n",
    "PBH_LogNormal = False\n",
    "PBH_Gaussian = False\n",
    "\n",
    "# Flags for fast execution\n",
    "\n",
    "Compute_SNRMat = False\n",
    "\n",
    "# Merger distribution parameters\n",
    "\n",
    "T_obs = 1. # Lisa or LIGO estimated years of observation\n",
    "efficiency = 1. # Lisa effective usefull time percentage for observations\n",
    "max_tc = 10000. # max years of coalescence time for a BBH mergine event\n",
    "frq_min = 3.e-5 # Hertz\n",
    "frq_max = 0.5 # Maximum frequency in hertz to which the LISA detector is sensitive\n",
    "frq_star = 1.e-2 # Value of the choosen frequency at which we estimate the SGWB to compare with other results\n",
    "# The total time used to generate the merging events by multipling for the rate of merging will be set to max_tc\n",
    "\n",
    "\n",
    "#General Constants \n",
    "\n",
    "c = 299792.46 # speed of light in Km/sec\n",
    "G = 6.674*(10.**(-11.)) # Gravitational constant in m^3⋅kg^−1⋅s^−2\n",
    "sol_mass = 1.988e30 # Value of the Solar Mass in Kg\n",
    "MPc = 3.08567758149137*1e22 # meters\n",
    "GPc = MPc*1e3 # meters\n",
    "h = 0.678\n",
    "H_0 = 67.8*1e3/MPc # Hubble constant in 1/(s)\n",
    "Omega_m = 0.3 # Matter density in our universe\n",
    "Omega_lambda = 0.7 # Cosmological constant density in our universe\n",
    "Omega_k = 0. # Curvature density in our universe\n",
    "rho_c = (3.*(H_0**2.))/(8.*np.pi*G) # Critical density in our universe\n",
    "year = 365.25*24*60*60 # Years in second \n",
    "    \n",
    "# Precision settings for the binned variables\n",
    "\n",
    "n_jobs = 80\n",
    "frq_res = 1e-6\n",
    "frq_prec = int((frq_max - frq_min)/frq_res) + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> FLAG selection section </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we have to decide which types of sources we wish to simulate in our SGWB, in order to use the standard _LIGO-Virgo_ fiducial mass function we have to set the _SOBBH_ flag on :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOBBH = True # If true, add the total SGWB strain produced by stellar origin binary black hole merging on the strain as estimated by LIGO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we wish to simulate PBH perturbations, instead, we have to choose between the two following different mass functions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PBH = True # If true, the FOM will be generated considering a PBH perturbation to the fiducial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PBH_LogNormal = True# This will simulate the Log Normal mass distribution for PBH described in ariv 2109.05836\n",
    "#PBH_Gaussian = True # This will simulate a Gaussian PBH mass distribution, that can be used to generalize a bit the standard monocromatic mass function for PBH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also decide to simulate the catalogue with a redshift evolving merging rate, by setting to true the Red_evol flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOBBH_Redevol = True # If true, the merging rate will evolve as a function of redshift, if false it will be assumed constant over the volume\n",
    "#SOBBH_RSpike = True # If true, generate a spike of merging rate in a small redshift region, the population will follow the standard SOBBH one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of PBH, instead, we are gonna generate the merging rate following a simple power law with $k = 1.4$ as in [V. Atal et al](https://arxiv.org/abs/2201.12218), for what concern the value of $R_0$, we will consider it to be a certain fraction $f$ of the original _SOBBH_ merging rate. This mode can be run by activating the flag _PBH_fRz_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PBH_fRz = True # If true, the merging rate would assumed to be the simple power law evolution of a fixed k, where the value of R0 would be given as a fraction f of the SOBBH one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alternatively, we can describe the evolution with redshift of the merging rate using the model by [S. S. Bavera et al](https://arxiv.org/pdf/2109.05836.pdf), this would describe its evolution as a power law of the _Hubble Time_ at redshift $z$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PBH_fRt = True # If true, the merging rate would assumed to be a powerlaw of the Hubble time at redshift z, where the value of R0 would be given as a fraction f of the SOBBH one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analytical SNR approximation can run using the following two modes for the inclination approximation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Inc_mode = 'max_i'  # Maximize the estimated SNR by assuming perfect inclination in respect to the detector\n",
    "#Inc_mode = 'avg_i' # Average the estimated SNR by integrating over all the possible inclinations in respect to the detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the sources, we can furthermore decide if we wish to plot their merging rates in function of $z$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot_Rz = True # If true, generate plots at the end of the simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also decide if recomputing the SNR matrix in the phase space or just use the one we already have as a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Compute_SNRMat = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Utility functions </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we are going to define some useful generical functions that will be needed to present the results.\n",
    "We will start with a function that can be used to convert matplotlib contour line to arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contour_verts(cn):\n",
    "    # Given a set of contour line, save them as a dictionary\n",
    "    contours = []\n",
    "    # for each contour line\n",
    "    for cc in cn.collections:\n",
    "        paths = []\n",
    "        # for each separate section of the contour line\n",
    "        for pp in cc.get_paths():\n",
    "            xy = []\n",
    "            # for each segment of that section\n",
    "            for vv in pp.iter_segments():\n",
    "                xy.append(vv[0])\n",
    "            paths.append(np.vstack(xy))\n",
    "        contours.append(paths)\n",
    "\n",
    "    return contours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Standard Cosmological Functions </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we'll need a function that allow us to convert from redshift to Gigaparsec :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a function to convert from Z to GPC using Hubble Law, in order to obtain the comoving distance\n",
    "\n",
    "z_max = 1.e8\n",
    "z_prec = 1000\n",
    "\n",
    "def H(z):\n",
    "    return np.sqrt((H_0**2.)*(Omega_m*((1. + z)**3.) + Omega_k*((1. + z)**2.) + Omega_lambda))\n",
    "\n",
    "def Z_to_Gpc(z):\n",
    "    \n",
    "    # Remove the commented part to use a linear approximation of the Hubble law for low z \n",
    "    \n",
    "    #if(zmax <= 0.5):\n",
    "    #    return ((z*c*(10**(-3)))/(H_0)) # only valid for z < 0.5\n",
    "    #else:\n",
    "        \n",
    "        span_z = np.linspace(0.,z,z_prec)\n",
    "        span_mz = 0.5*(span_z[1::] + span_z[:-1:])\n",
    "        \n",
    "        # Beware, would fail if the span z is created in logarithmic scale !\n",
    "        \n",
    "        Int_Z = c*(10**(-3))*simpson(1./(H(span_mz)*(MPc/1.e3)), span_mz, axis=0)\n",
    "    \n",
    "        return Int_Z\n",
    "    \n",
    "def Z_to_HubbleTime(z):\n",
    "    \n",
    "    span_z = np.logspace(np.log10(z),np.log10(z_max),z_prec)\n",
    "    span_mz = 0.5*(span_z[1::] + span_z[:-1:])\n",
    "        \n",
    "    # Beware, would fail if the span z is created in logarithmic scale !\n",
    "        \n",
    "    \n",
    "    Int_Z = simpson(1./(H(span_mz)*(1. + span_mz)), span_mz, axis=0)\n",
    "    \n",
    "    return Int_Z\n",
    "    \n",
    "t_0 = Z_to_HubbleTime(1.e-12) # Can't put 0 as the logarithmic scale would fail        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we also need a function that estimates the differential comoving volume in function of the redshift :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the following function, the differential comoving volume in function of the redshift will be estimated as a spherical surface, it need to be integrated over dr to obtain the real volume \n",
    "\n",
    "def DeVC(z, Delta_z):\n",
    "    r = dist_func(z)\n",
    "    z_2 = z + 0.5*Delta_z\n",
    "    z_1 = z_2 - Delta_z\n",
    "    Delta_r = dist_func(z_2) - dist_func(z_1)\n",
    "    return ((4.*np.pi*(r**2.)*Delta_r)/Delta_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another recurring parameter for inspiralling events is the Chirp Mass, given the mass of the two events involved in the binary merging :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that return the Chirp Mass of a binary merging event\n",
    "\n",
    "def ChirpMass(m1,m2): \n",
    "   return ((m1*m2)**(3./5.))/((m1+m2)**(1./5.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "together with the effective spin :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that given the spin and spin tilt gives the effective spin\n",
    "\n",
    "def EffectiveSpin(m1, m2, a1, a2, st_a1, st_a2):\n",
    "    res = (m1*a1*cos(st_a1))/(m1 + m2) + (m2*a2*cos(st_a1))/(m1 + m2) # Hope so, better to double check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent the signal in units of omega, we are gonna need to convert our strain from units of _h_, _hc_, or _Flux_ to units of $\\Omega_{gw}$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_to_Omega(ran_frq, spectrum):\n",
    "    # ran_frq and spectrum need to have same shape\n",
    "    return ((4*((h*np.pi)**2.)*(ran_frq**3.)*spectrum)/(3.*(H_0**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hc_to_Omega(ran_frq, spectrum):\n",
    "    # ran_frq and spectrum need to have same shape\n",
    "    return ((2*((h*np.pi)**2.)*(ran_frq**2.)*spectrum)/(3.*(H_0**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Flux_to_Omega(ran_frq, Flux):\n",
    "    # Flux need to be a constant expressing the whole integrated flux in function of z and m\n",
    "    return ((ran_frq**(2./3.))/(rho_c*(c*1e3)**3))*Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, we may define the energy loss during the inspiral phase, the procedure implemented is described in [P. Ajith et al.](https://arxiv.org/abs/0909.2867), even though in the LISA case we can use the assumption that all the waveforms appearing in detector, are in the pre-merger phase.\n",
    "We have :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dE_dnu(m1, m2, freq, a1 = -1, a2 =-1, st_a1 = -1, st_a2 = -1):\n",
    "    # Compute the energy dispersed during an inspiral phase to a certain post-newtonian order in the pre-merger approximation\n",
    "    # If the 4 parameters describing the spin configuration are not given, would automatically use only the first post-newtonian term\n",
    "    \n",
    "    Ch_M = ChirpMass(m1,m2)\n",
    "    eta = m1*m2/(Ch_M**2)\n",
    "    nu_prime = (np.pi*Ch_M*sol_mass*G*freq/c**3)**(1./3.)\n",
    "    \n",
    "    alpha_2 = -(323/224) + (451/168)*eta\n",
    "    if(a1 == -1 or a2 == -1 or st_a1 == -1 or st_a2 == -1):\n",
    "        alpha_3 = 0\n",
    "    else:\n",
    "        chi_spin = EffectiveSpin(m1, m2, a1, a2, st_a1, st_a2) \n",
    "        alpha_3 = ((27/8) - (11/6)*eta)*chi_spin\n",
    "    f1 = 1 + alpha_2*nu_prime**2 + alpha_3*nu_prime**3\n",
    "    \n",
    "    res = (((G*np.pi)*(Ch_M**(5./3.)))/3)*(freq**(-1./3.))*(f1**2)\n",
    "    \n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while the total spectrum in Omega given by any BH channel expressed in energy spectral density, can be generally described using :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpectralDens_to_OmegaGW(freq, F_nu):\n",
    "    res = (freq/(rho_c * c**3))*F_nu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> LISA sensitivity curve </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we are going to generate the LISA sensitivity curve, in order to compare our result with the properties of the instrument.\n",
    "The shape of the sensitivity curve in units of S can be defined using the following function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the value of the sensitivity curve S_h given the frequency\n",
    "\n",
    "def get_SciRD(freq):\n",
    "    S_2 = 3.6*10.**(-41.) #1/Hz\n",
    "    S_1 = 5.76*(1. + (0.0004/freq)**2.)*10.**(-48.) # 1/(Hz*s^4)\n",
    "    S_R = 1. + (freq/0.025)**2.\n",
    "    S_h = (10./3.)*S_R*((S_1/(2.*np.pi*freq)**4.) + S_2)\n",
    "    return S_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> SOBBH LIGO All Channels SGWB </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to initialize all the objects needed to compute the Stellar Origin Binary Black Hole merging(SOBBHm) SGWB.\n",
    "The probability distribution implemented for the variables of the events, will be taken from [B. P. Abbott T1](https://arxiv.org/abs/1811.12940), [B. P. Abbott T2](https://arxiv.org/abs/2010.14533)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> SOBBH - Characteristic strain functions </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The characteristic strain is given by :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to estimate the characteristic strain for SOBBHm events\n",
    "\n",
    "SGWB_zmin = 1.e-2\n",
    "SGWB_zmax = 1.e5\n",
    "SGWB_zprec = 10000\n",
    "\n",
    "def SOBBH_hcsqrd(frq, SOBBH_IntFac):\n",
    "    return ((4.*G**(5./3.))/(3.*(np.pi**(1./3.))*(c*10**3)**2))*(frq**(-4./3.))*SOBBH_IntFac\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> SOBBH - Mass distribution functions </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining the probability distribution in function of the masses.\n",
    "\n",
    "We have :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power law + Peak Mass Model of the paper arxiv 2010.14533\n",
    "\n",
    "    \n",
    "# Mass Distribution parameters (values taken from the results of arxiv 2111.03634)\n",
    "\n",
    "SOBBH_m = 2.5 # + 0.67 - 0.44  minimum mass allowed by the popolation inference \n",
    "SOBBH_M = 100. # Solar Masses, taken from the prior of the paper as no real higher mass cutoff was estimated !\n",
    "SOBBH_massprec = 500 # Binning density for the masses\n",
    "SOBBH_alpha = 3.4 # + 0.58 - 0.49 Power law index\n",
    "SOBBH_betaq = 1.1 # + 1.8 - 1.3  index for m2 power law in q\n",
    "SOBBH_deltam = 7.8 #+ 1.9 - 4.0  used for the low mass smoothing function, generate peak at delta_m + m_min\n",
    "SOBBH_lambdapeak = 0.039 # + 0.058 - 0.026 Intensity of the gaussian peak\n",
    "SOBBH_mum = 34 # + 2.3 - 3.8 Location of the Gaussian peak in Solar Masses\n",
    "SOBBH_sigmam = 5.09 # +4.28 - 4.34 Solar Masses, taken from arxiv 2010.14533 as no additional claim was made on last paper\n",
    "\n",
    "# Defining of the smoothing function for m close to the minimimum mass\n",
    "\n",
    "def SOBBH_MassSmoothing(m, SOBBH_m, SOBBH_deltam):\n",
    "    if(m < SOBBH_m):\n",
    "        return 0.\n",
    "    else:\n",
    "        if(m >= (SOBBH_m + SOBBH_deltam)):\n",
    "            return 1.\n",
    "        else:\n",
    "            factor = np.exp((SOBBH_deltam/(m - SOBBH_m)) + (SOBBH_deltam/(m - SOBBH_m - SOBBH_deltam)))\n",
    "            return 1./(factor + 1.)\n",
    "\n",
    "# Defining a normalized power law distribution function, needed for the final distribution function        \n",
    "\n",
    "def SOBBH_MassPowLaw(m, SOBBH_m, SOBBH_M, SOBBH_alpha, SOBBH_PLnorm):\n",
    "    if(SOBBH_m < m < SOBBH_M):\n",
    "        return (1./SOBBH_PLnorm)*(m**(-SOBBH_alpha))\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "# Estimating the Phase space of the Power law distribution using trapezoidal integration\n",
    "\n",
    "def SOBBH_PowerLawPS(SOBBH_ranm1, SOBBH_m, SOBBH_M, SOBBH_alpha):\n",
    "\n",
    "    ris = 0.\n",
    "\n",
    "    for i in range(len(SOBBH_ranm1)- 1):\n",
    "        if(SOBBH_ranm1[i] >= SOBBH_m and SOBBH_ranm1[i] <= SOBBH_M):\n",
    "            mid_m1 = 0.5*(SOBBH_ranm1[i + 1] + SOBBH_ranm1[i])\n",
    "            ris +=  (SOBBH_ranm1[i + 1] - SOBBH_ranm1[i])*(np.power(mid_m1, (-SOBBH_alpha)))\n",
    "\n",
    "        return ris\n",
    "\n",
    "\n",
    "# Defining a Gaussian distribution of the mass, needed for the final distribution function\n",
    "\n",
    "def SOBBH_MassGauss(m, SOBBH_mum, SOBBH_sigmam, SOBBH_GSnorm):\n",
    "    return ((1./(SOBBH_sigmam*np.sqrt(2.*np.pi)))*np.exp(-0.5*((m-SOBBH_mum)/SOBBH_sigmam)**2.))*1./SOBBH_GSnorm\n",
    "\n",
    "def SOBBH_GaussPS(SOBBH_ranm1, SOBBH_m, SOBBH_M, SOBBH_mum, SOBBH_sigmam):\n",
    "\n",
    "    ris = 0.\n",
    "\n",
    "    for i in range(len(SOBBH_ranm1)- 1):\n",
    "        if(SOBBH_ranm1[i] >= SOBBH_m and SOBBH_ranm1[i] <= SOBBH_M):\n",
    "            mid_m1 = 0.5*(SOBBH_ranm1[i + 1] + SOBBH_ranm1[i])\n",
    "            ris +=  (SOBBH_ranm1[i + 1] - SOBBH_ranm1[i])*((1./(SOBBH_sigmam*np.sqrt(2.*np.pi)))\\\n",
    "                    *np.exp(-0.5*((mid_m1-SOBBH_mum)/SOBBH_sigmam)**2.))\n",
    "\n",
    "    return ris\n",
    "\n",
    "# Defining the normalization constant for the q dependancy of the total mass distribution\n",
    "\n",
    "def SOBBH_P2PS(SOBBH_ranm2, SOBBH_betaq, SOBBH_m, SOBBH_deltam):\n",
    "\n",
    "    q_norm = np.linspace(0,1,len(SOBBH_ranm2))\n",
    "\n",
    "    for i in range(len(SOBBH_ranm2) - 1):\n",
    "\n",
    "        q_norm[i] = 0.\n",
    "\n",
    "        for j in range(i + 1):\n",
    "\n",
    "            q_norm[i] += ((0.5*(SOBBH_ranm2[j] + SOBBH_ranm2[j + 1])/(0.5*(SOBBH_ranm2[i] + SOBBH_ranm2[i + 1])))\\\n",
    "                         **(SOBBH_betaq))*(SOBBH_ranm2[j + 1] - SOBBH_ranm2[j])*\\\n",
    "                         SOBBH_MassSmoothing(0.5*(SOBBH_ranm2[j] + SOBBH_ranm2[j + 1]), SOBBH_m, SOBBH_deltam)\n",
    "        \n",
    "    q_norm[len(SOBBH_ranm2) - 1] = q_norm[len(SOBBH_ranm2) - 2]\n",
    "\n",
    "    return q_norm   \n",
    "\n",
    "\n",
    "# Defining the proper Mass distribution function\n",
    "\n",
    "def SOBBH_MassDistr(m1, m2, SOBBH_m, SOBBH_M, SOBBH_alpha, SOBBH_betaq, SOBBH_deltam, SOBBH_lambdapeak, SOBBH_mum, SOBBH_sigmam, SOBBH_PLnorm, SOBBH_GSnorm, SOBBH_qnorm, SOBBH_MassPS):\n",
    "\n",
    "    if(m1 >= m2):\n",
    "        return ((1. - SOBBH_lambdapeak)*SOBBH_MassPowLaw(m1, SOBBH_m, SOBBH_M, SOBBH_alpha, SOBBH_PLnorm) + \\\n",
    "                SOBBH_lambdapeak*SOBBH_MassGauss(m1, SOBBH_mum, SOBBH_sigmam, SOBBH_GSnorm))*\\\n",
    "                SOBBH_MassSmoothing(m1, SOBBH_m, SOBBH_deltam)*\\\n",
    "                ((m2/m1)**(SOBBH_betaq))*(1./SOBBH_qnorm)*\\\n",
    "                SOBBH_MassSmoothing(m2, SOBBH_m, SOBBH_deltam)*(1./SOBBH_MassPS)\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "# Estimating the Phase space for the Model C Mass distribution function using trapezoidal integration\n",
    "\n",
    "def SOBBH_ModCPS(SOBBH_ranm1, SOBBH_ranm2, SOBBH_m, SOBBH_M, SOBBH_alpha, SOBBH_betaq, SOBBH_deltam, SOBBH_lambdapeak, SOBBH_mum, SOBBH_sigmam, SOBBH_PLnorm, SOBBH_GSnorm, SOBBH_qnorm, SOBBH_MassPS):\n",
    "\n",
    "    ris = 0.\n",
    "\n",
    "    for i in range(len(SOBBH_ranm1)- 1):\n",
    "        for j in range(len(SOBBH_ranm2)- 1):\n",
    "                 if(SOBBH_ranm1[i] >= SOBBH_ranm2[j]):\n",
    "                    mid_m1 = 0.5*(SOBBH_ranm1[i + 1] + SOBBH_ranm1[i])\n",
    "                    mid_m2 = 0.5*(SOBBH_ranm2[j + 1] + SOBBH_ranm2[j])\n",
    "                    q = mid_m2/mid_m1 \n",
    "                    ris +=  (SOBBH_ranm1[i + 1] - SOBBH_ranm1[i])*(SOBBH_ranm2[j + 1] - SOBBH_ranm2[j])*\\\n",
    "                    ((1. - SOBBH_lambdapeak)*SOBBH_MassPowLaw(mid_m1, SOBBH_m, SOBBH_M, SOBBH_alpha, SOBBH_PLnorm)\\\n",
    "                    + SOBBH_lambdapeak*SOBBH_MassGauss(mid_m1, SOBBH_mum, SOBBH_sigmam, SOBBH_GSnorm))\\\n",
    "                    *SOBBH_MassSmoothing(mid_m1, SOBBH_m, SOBBH_deltam)*(q**(SOBBH_betaq))\\\n",
    "                    *(1./SOBBH_qnorm[i])*SOBBH_MassSmoothing(mid_m2, SOBBH_m, SOBBH_deltam)*(1./SOBBH_MassPS)\n",
    "\n",
    "    return ris\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> SOBBH - Redshift dependent statistic </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may now define, the various implemented merging rates as a function of the redshift _z_ as :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for the merging rate as described in the paper arxiv 2010.14533, the flag Red_evol will decide if adopting a merging rate the evolve with redshift (true) or not (false)\n",
    "\n",
    "if SOBBH:\n",
    "    SOBBH_z = 1.e-5 # to avoid SNR divergence due to extremely close events\n",
    "    SOBBH_Zlog = 0.1 # max z value generated in log scale\n",
    "    SOBBH_Zlin = 10.0 # max z value generated in lin scale\n",
    "    SOBBH_zprec = 250 # Binning density for the redshift\n",
    "\n",
    "SOBBH_k = 2.7 # + 1.8 - 1.9  VALID FOR REDSHIFT EVOLVING POWER LAW + PEAK MODEL MASS DISTRIBUTION, total agreement with SFR\n",
    "SOBBH_CorrRz = (((1. + 0.2)**SOBBH_k)/(1. + ((1. + 0.2)/2.9)**(SOBBH_k + 2.9)))**(-1) # Normalization factor estimated at z = 0.2\n",
    "    \n",
    "# Defining the value of R0, the 0 index will have the value for redshift evolution merging rate, the 1 index would have the one for constant merging rate\n",
    "\n",
    "SOBBH_R0 = {}\n",
    "SOBBH_R0[0] = 28.1/(year*GPc**3.)# +14.8 - 10.0 GPC⁻³ yr^⁻¹ Value of the merging rate fitted at z = 0.2\n",
    "SOBBH_R0[1] = 23.9/(year*GPc**3.) # +14.9 - 8.6 m^-3 s^-1 Middle value fitted using a Power Law + Peak mass model and a non evolving merging rate\n",
    "\n",
    "def SOBBH_R(z):\n",
    "    if(SOBBH_Redevol):\n",
    "        # This merging rate was interpolated by Angelo Ricciardone and Daniel Figueroa based on arxiv 2010.14533 and arxiv 1907.12562\n",
    "        return SOBBH_R0[0]*SOBBH_CorrRz*((1. + z)**SOBBH_k)/(1. + ((1. + z)/2.9)**(SOBBH_k + 2.9))\n",
    "    else:\n",
    "        return SOBBH_R0[1]\n",
    "\n",
    "# If we wish to generate just a spike of events at a certain redshift range coming from a merging rate with fixed amplitude, we fix the following        \n",
    "        \n",
    "if SOBBH_RSpike:\n",
    "    # These variables would set the location of the spike in the redshift range\n",
    "    SOBBH_Rzmin = 2.\n",
    "    SOBBH_Rzmax = 10.\n",
    "    SOBBH_zprec = 80\n",
    "    # These variables, will set the phase space to span in order to obtain the figure of merit \n",
    "    SOBBH_SpikeAmplMin = 0.\n",
    "    SOBBH_SpikeAmplMax = 10000.\n",
    "    SOBBH_SpikeAmplPrec = 500\n",
    "    # This will be the value of the amplitude of the merging rate for the constant spike\n",
    "    SOBBH_SpikeAmpl = 1.\n",
    "    \n",
    "    def SOBBH_R(z):\n",
    "        # Pass the amplitude in units of 1/[yr*GPc], tipically the value is between [1, 200]\n",
    "        return SOBBH_SpikeAmpl/(year*(GPc**3.))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> SOBBH - Number density of events</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may finally define the distribution function for the number of events,in particular let's start with the function that describes the merging rate dependancy on the reference frame time: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DtrDz(z):\n",
    "    ris = 1./(H_0*(1. + z)*np.sqrt(Omega_m*((1. + z)**3.) + Omega_k*((1. + z)**2.) + Omega_lambda))\n",
    "    return ris\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can now integrate the mass and redshift dependant factor in order to get a constant that will multiply the frequency dependance of the characteristic strain function.\n",
    "After putting together all the integral dependant factors, we just have to integrate :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SOBBH :\n",
    "    def SOBBH_IntND(i):\n",
    "        \n",
    "        ris = 0.\n",
    "        \n",
    "        if ((i*10)%len(SOBBH_ranz) == 0) :\n",
    "            print('Percentage of completition : ',(i*100.)/(len(SOBBH_ranz)), '%')\n",
    "                \n",
    "        for j in range(len(SOBBH_ranm1)-1):\n",
    "            for k in range(j + 1):\n",
    "                deltas = (SOBBH_ranz[i + 1] - SOBBH_ranz[i])*(SOBBH_ranm1[j + 1] - SOBBH_ranm1[j])*(SOBBH_ranm2[k + 1] - SOBBH_ranm2[k])\n",
    "                ris += deltas*SOBBH_R(0.5*(SOBBH_ranz[i + 1] + SOBBH_ranz[i]))*\\\n",
    "                            SOBBH_MassDistr(0.5*(SOBBH_ranm1[j + 1] + SOBBH_ranm1[j]), 0.5*(SOBBH_ranm2[k + 1] + SOBBH_ranm2[k]),\\\n",
    "                                           SOBBH_m, SOBBH_M, SOBBH_alpha, SOBBH_betaq, SOBBH_deltam, \\\n",
    "                                           SOBBH_lambdapeak, SOBBH_mum, SOBBH_sigmam, SOBBH_PLnorm, SOBBH_GSnorm, \\\n",
    "                                           SOBBH_qnorm[j], SOBBH_MassPS)*\\\n",
    "                            DtrDz(0.5*(SOBBH_ranz[i + 1] + SOBBH_ranz[i]))*\\\n",
    "                            ((ChirpMass(0.5*(SOBBH_ranm1[j + 1] + SOBBH_ranm1[j]),\\\n",
    "                                        0.5*(SOBBH_ranm2[k + 1] + SOBBH_ranm2[k]))*sol_mass)**(5./3.))\\\n",
    "                            /((1. + 0.5*(SOBBH_ranz[i + 1] + SOBBH_ranz[i]))**(1./3.)) \n",
    "                        \n",
    "        return [0.5*(SOBBH_ranz[i + 1] + SOBBH_ranz[i]),ris]\n",
    "                               \n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of events predicted at each $z$ can finally be obtained using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SOBBH_NDistrib(z, m1, m2, Delta_z, SOBBH_qnorm):\n",
    "    n = SOBBH_R(z)*(year*GPc**3.)*DeVC(z, Delta_z)*(T_obs/(1. + z)) \\\n",
    "        *SOBBH_MassDistr(m1, m2, SOBBH_m, SOBBH_M, SOBBH_alpha, SOBBH_betaq, SOBBH_deltam, SOBBH_lambdapeak, SOBBH_mum, SOBBH_sigmam, SOBBH_PLnorm, SOBBH_GSnorm, SOBBH_qnorm, SOBBH_MassPS)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> PBH population functions </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to initialize the population functions needed to simulate the _Primordial Black Holes (PBH)_ SGWB.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> PBH - Merging Rates </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute a PBH perturbation analysis, we are gonna define the PBH merging rate as a fraction of the fiducial LIGO merging rate.\n",
    "We start by defining the model presented in [V. Atal et al.](https://arxiv.org/abs/2201.12218) evolving with a simple power broken power law having $k = 1.1$ before $z_*$ and $k = 1.4$ after. \n",
    "The model is as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PBH_fRz :\n",
    "    \n",
    "    PBH_zmin = 0.5 # minimum value of the PBH merging rate\n",
    "    PBH_zmax = 5.0 # max z value generated in lin scale\n",
    "    PBH_zprec = 200 # Binning density for the redshift\n",
    "\n",
    "    # Defining the value of R0, the 0 index will have the value for redshift evolution merging rate, the 1 index would have the one for constant merging rate\n",
    "\n",
    "    PBH_R0 = 28.1/(year*GPc**3.) # +14.8 - 10.0 GPC⁻³ yr^⁻¹ Value of the merging rate fitted in at z = 0.2 in ligo population inference paper arxiv2111.03634\n",
    "    PBH_CorrfRz = 1./(1. + 0.2)**2.7 # normalization factor needed to express the value of the LIGO merging rate in z=0\n",
    "    \n",
    "    def PBH_fR(z,f):\n",
    "        if(z <= 1.):\n",
    "            PBH_k = 1.1 # Value taken from arxiv 2201.12218, valid for small z !!\n",
    "            return f*PBH_R0*PBH_CorrfRz*((1. + z)**PBH_k)\n",
    "        else:\n",
    "            PBH_k = 1.4 # Value taken from arxiv 2201.12218, valid for high z !!\n",
    "            PBH_R1_corr = f*PBH_R0*PBH_CorrfRz*(((2.)**PBH_k) - ((2.)**1.1))\n",
    "            return f*PBH_R0*PBH_CorrfRz*((1. + z)**PBH_k) - PBH_R1_corr\n",
    "    \n",
    "    def PBH_fRVec(z,f, tilt_low=1.1, tilt_high=1.4):\n",
    "    \n",
    "        to_ret = f*PBH_R0*PBH_CorrfRz\n",
    "        z_fac  = (1. + z)**tilt_low\n",
    "        z_fac[z > 1.] = (1. + z[z > 1])**tilt_high - (((2.)**tilt_high) - ((2.)**tilt_low))\n",
    "        return to_ret*z_fac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alternatively, we can use the same model described by [S. S. Bavera et al](https://arxiv.org/pdf/2109.05836.pdf) for the redshift evolution of the merging rate. The amplitude of the perturbation can still be parametrized using the $fR$ approach as in the previous model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PBH_fRt:\n",
    "    PBH_zmin = 0.5 # minimum value of the PBH merging rate\n",
    "    PBH_zmax = 5.0 # max z value generated in lin scale\n",
    "    PBH_zprec = 200 # Binning density for the redshift\n",
    "\n",
    "    PBH_R0 = 28.1/(year*GPc**3.) # +14.8 - 10.0 GPC⁻³ yr^⁻¹ Value of the merging rate fitted in at z = 0.2 in ligo population inference paper arxiv2111.03634\n",
    "    PBH_CorrfRz = 1./(1. + 0.2)**2.7 # normalization factor needed to express the value of the LIGO merging rate in z=0\n",
    "    \n",
    "    def PBH_fR(z,f):\n",
    "        return f*PBH_R0*PBH_CorrfRz*((t_z(z)/t_0)**(-34./37.))\n",
    "    \n",
    "    def PBH_fRVec(z,f):\n",
    "        return f*PBH_R0*PBH_CorrfRz*((t_z(z)/t_0)**(-34./37.))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> PBH - Gaussian Mass Distribution </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a Gaussian mass distribution for PBH as :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PBH_Gaussian:\n",
    "    PBH_m = 2.5 # Solar Masses. Minimum value assumed for the PBH mass\n",
    "    PBH_M = 100. # Solar Masses. Maximum value assumed for the PBH mass\n",
    "    PBH_massprec = 150 # Binning density for the mass range\n",
    "    PBH_pdfmspan = np.linspace(0., 100., 200) # this span will be needed to compute the figures of merit\n",
    "    PBH_mu = 5. # mean of the Gaussian distribution\n",
    "    PBH_sigmam = 1. # sigma of the variance distribution\n",
    "    PBH_sigmamspan = [1. ,5. ,10. ,25.] # Values of sigma_m to be spanned by the simulation\n",
    "    \n",
    "    # We use the following distribution for the mass, this tend to a monochromatic mass function for small values of sigma, yet it can be used to generalize the result to a wider subset of cases\n",
    "    def PBH_MassGauss(m, PBH_mu, PBH_sigmam, PBH_GSnorm):\n",
    "        return ((1./(PBH_sigmam*np.sqrt(2.*np.pi)))*np.exp(-0.5*((m-PBH_mu)/PBH_sigmam)**2.))*1./PBH_GSnorm\n",
    "    \n",
    "    # This function is to estimate the normalization constant\n",
    "    def PBH_GaussPS(PBH_ranm, PBH_mu, PBH_sigmam):\n",
    "\n",
    "        PBH_midm = 0.5*(PBH_ranm[1::] + PBH_ranm[:-1:])\n",
    "\n",
    "        ris =  simpson(PBH_MassGauss(PBH_midm, PBH_mu, PBH_sigmam, 1.), PBH_midm)\n",
    "            \n",
    "        return ris\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> PBH - Log-Normal Mass Distribution </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a Log-Normal mass distribution for PBH as described in the paper by [S. S. Bavera et al ](https://arxiv.org/abs/2109.05836):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PBH_LogNormal:\n",
    "    # We use the following distribution for the mass\n",
    "    PBH_m = 2.5 # Solar Masses. Minimum value assumed for the PBH mass\n",
    "    PBH_M = 100. # Solar Masses. Maximum value assumed for the PBH mass\n",
    "    PBH_massprec = 150 # Binning density for the mass range\n",
    "    PBH_pdfmspan = np.linspace(0, 100., 200) # this span will be needed to compute the figures of merit\n",
    "    PBH_Mc = 34.54 # Solar masses, taken from the main paper by Bavera\n",
    "    PBH_sigmamn = 0.41 # Taken from the main paper by Bavera\n",
    "    PBH_sigmamnspan = [0.1 ,0.5 ,1. ,5.] # Values of sigma_m to be spanned by the simulation\n",
    "    \n",
    "    def PBH_MassLNorm(m, PBH_Mc, PBH_sigmamn, PBH_LNnorm):\n",
    "        return (1./(np.sqrt(2*np.pi)*PBH_sigmamn*m))*np.exp(-(np.log(m/PBH_Mc)**2)/(2*PBH_sigmamn**2))*1./PBH_LNnorm\n",
    "    \n",
    "    # This function is to estimate the normalization constant\n",
    "    def PBH_LNnormPS(PBH_ranm, PBH_Mc, PBH_sigmamn):\n",
    "        \n",
    "        PBH_midm = 0.5*(PBH_ranm[1::] + PBH_ranm[:-1:])\n",
    "\n",
    "        ris =  simpson(PBH_MassLNorm(PBH_midm, PBH_Mc, PBH_sigmamn, 1.), PBH_midm)\n",
    "            \n",
    "        return ris\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> PBH - Number density of events</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain the number of resolvable events at each z for the fiducial and sub-populations, we need first of all to know the SNR for an event in function of its parameter space $(z, m1, m2)$.\n",
    "We will hence define a function that gives the entry at each bin in the parameter space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PBH :\n",
    "    def AnalSNR_ParamSpace(z, mat):\n",
    "    \n",
    "        if (0.5*(PBH_ranz[z + 1] + PBH_ranz[z]) >= 0.5):\n",
    "            for j in range(len(PBH_ranm1) - 1):\n",
    "                for k in range(j + 1):\n",
    "                    vals = IMRPhenomD_AnalSNR(0.5*(PBH_ranz[z + 1] + PBH_ranz[z]), 0.5*(PBH_ranm1[j + 1] + PBH_ranm1[j]), 0.5*(PBH_ranm2[k + 1] + PBH_ranm2[k]))\n",
    "                    delta_val = pd.DataFrame([[z, j, k, vals[0], vals[1]],], columns = ['idx_z', 'idx_m1', 'idx_m2', 'aplusLIGO_val', 'ET_val'])\n",
    "                    mat.append(delta_val)                \n",
    "        \n",
    "        if ((z*10)%(len(PBH_ranz) - 1) == 0) :\n",
    "            print('Percentage of completition : ',(z*100.)/(len(PBH_ranz) - 1), '%', flush=True)\n",
    "        \n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PBH :\n",
    "    def PBHvsFid_ResSrc(i, mat):\n",
    "        #Initializing instance variables\n",
    "        ris = 0.\n",
    "        idx_shared = []\n",
    "        LIGO_idxtofill = [count for count in range(len(PBH_frange))]\n",
    "        ET_idxtofill = [count for count in range(len(PBH_frange))]\n",
    "        \n",
    "        # Estimating the normalization constant for the Gaussian Case \n",
    "        \n",
    "        if PBH_Gaussian:\n",
    "            PBH_GSnorm = PBH_GaussPS(PBH_ranm1, 0.5*(PBH_pdfmspan[i + 1] + PBH_pdfmspan[i]), PBH_sigmam)\n",
    "        \n",
    "        # Estimating the normalization constant for the LogNormal case\n",
    "        \n",
    "        if PBH_LogNormal:\n",
    "            PBH_LNnorm = PBH_LNnormPS(PBH_ranm1, 0.5*(PBH_pdfmspan[i + 1] + PBH_pdfmspan[i]), PBH_sigmamn)\n",
    "        \n",
    "        for z in range(len(PBH_ranz) - 1):\n",
    "            if (0.5*(PBH_ranz[z + 1] + PBH_ranz[z]) >= 0.5):\n",
    "                if (len(LIGO_idxtofill) > 0 or len(ET_idxtofill) > 0 ):                \n",
    "                    \n",
    "                    # Initializing the resolvable sources at distance z for fiducial and subpopulation\n",
    "                    \n",
    "                    LIGO_fidres = 0.\n",
    "                    LIGO_pertres = PBH_frange*0.\n",
    "                    ET_fidres = 0.\n",
    "                    ET_pertres = PBH_frange*0.\n",
    "                    \n",
    "                    # Spanning over all the mass couples\n",
    "            \n",
    "                    for j in range(len(PBH_ranm1) - 1):\n",
    "                        for k in range(j + 1):\n",
    "                            deltas = (PBH_ranz[z + 1] - PBH_ranz[z])*(PBH_ranm1[j + 1] - PBH_ranm1[j])*(PBH_ranm2[k + 1] - PBH_ranm2[k])\n",
    "\n",
    "                            if LIGO_SNRs[z][j][k] >= 8.:\n",
    "\n",
    "                                LIGO_fidres += SOBBH_NDistrib(0.5*(PBH_ranz[z + 1] + PBH_ranz[z]), 0.5*(PBH_ranm1[j + 1] + PBH_ranm1[j]), 0.5*(PBH_ranm2[k + 1] + PBH_ranm2[k]), (PBH_ranz[z + 1] - PBH_ranz[z]), q_norm[j])*deltas\n",
    "\n",
    "                                if PBH_Gaussian:\n",
    "                                    LIGO_pertres += PBH_NDistrib(0.5*(PBH_ranz[z + 1] + PBH_ranz[z]), 0.5*(PBH_ranm1[j + 1] + PBH_ranm1[j]), 0.5*(PBH_ranm2[k + 1] + PBH_ranm2[k]), (PBH_ranz[z + 1] - PBH_ranz[z]), PBH_frange, PBH_GSnorm)*deltas\n",
    "\n",
    "                                if PBH_LogNormal:\n",
    "                                    LIGO_pertres += PBH_NDistrib(0.5*(PBH_ranz[z + 1] + PBH_ranz[z]), 0.5*(PBH_ranm1[j + 1] + PBH_ranm1[j]), 0.5*(PBH_ranm2[k + 1] + PBH_ranm2[k]), (PBH_ranz[z + 1] - PBH_ranz[z]), PBH_frange, PBH_LNnorm)*deltas\n",
    "\n",
    "                            if ET_SNRs[z][j][k] >= 8.:\n",
    "\n",
    "                                ET_fidres += SOBBH_NDistrib(0.5*(PBH_ranz[z + 1] + PBH_ranz[z]), 0.5*(PBH_ranm1[j + 1] + PBH_ranm1[j]), 0.5*(PBH_ranm2[k + 1] + PBH_ranm2[k]), (PBH_ranz[z + 1] - PBH_ranz[z]), q_norm[j])*deltas\n",
    "\n",
    "                                if PBH_Gaussian:\n",
    "                                    ET_pertres += PBH_NDistrib(0.5*(PBH_ranz[z + 1] + PBH_ranz[z]), 0.5*(PBH_ranm1[j + 1] + PBH_ranm1[j]), 0.5*(PBH_ranm2[k + 1] + PBH_ranm2[k]), (PBH_ranz[z + 1] - PBH_ranz[z]), PBH_frange, PBH_GSnorm)*deltas\n",
    "\n",
    "                                if PBH_LogNormal:\n",
    "                                    ET_pertres += PBH_NDistrib(0.5*(PBH_ranz[z + 1] + PBH_ranz[z]), 0.5*(PBH_ranm1[j + 1] + PBH_ranm1[j]), 0.5*(PBH_ranm2[k + 1] + PBH_ranm2[k]), (PBH_ranz[z + 1] - PBH_ranz[z]), PBH_frange, PBH_LNnorm)*deltas\n",
    "\n",
    "\n",
    "                    # Saving the index for the aplusLIGO case\n",
    "                    \n",
    "                    idx_mod = [i for i,v in enumerate(LIGO_pertres) if v > 3.*np.sqrt(LIGO_fidres)]\n",
    "                    idx_shared = list(set(idx_mod) & set(LIGO_idxtofill))\n",
    "                    if len(idx_shared) > 0:\n",
    "                        for idx in idx_shared:\n",
    "                            delta_val = pd.DataFrame([[0, i, idx, 0.5*(PBH_ranz[z + 1] + PBH_ranz[z])],], columns = ['Detector', 'idx_pdfm', 'idx_Rf', 'v'])\n",
    "                            mat.append(delta_val)\n",
    "                        LIGO_idxtofill = [val for val in LIGO_idxtofill if val not in idx_shared]\n",
    "                    \n",
    "                   # Saving the index for the ET case\n",
    "                    \n",
    "                    idx_mod = [i for i,v in enumerate(ET_pertres) if v > 3.*np.sqrt(ET_fidres)]\n",
    "                    idx_shared = list(set(idx_mod) & set(ET_idxtofill))\n",
    "                    if len(idx_shared) > 0:\n",
    "                        for idx in idx_shared:\n",
    "                            delta_val = pd.DataFrame([[1, i, idx, 0.5*(PBH_ranz[z + 1] + PBH_ranz[z])],], columns = ['Detector', 'idx_pdfm', 'idx_Rf', 'v'])\n",
    "                            mat.append(delta_val)\n",
    "                        ET_idxtofill = [val for val in ET_idxtofill if val not in idx_shared]\n",
    "                else:\n",
    "                    break\n",
    "        \n",
    "        if ((i*10)%len(PBH_pdfmspan) == 0) :\n",
    "            print('Percentage of completition : ',(i*100.)/(len(PBH_pdfmspan)), '%', flush=True)\n",
    "        \n",
    "                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of predicted events for the _PBH_ subpopulation at each $z$, can again be obtained using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PBH_NDistrib(z, m1, m2, Delta_z, f, PBH_norm):\n",
    "    if PBH_Gaussian:\n",
    "        n = PBH_fR(z,f)*(year*GPc**3.)*DeVC(z, Delta_z)*(T_obs /(1. + z)) \\\n",
    "            *PBH_MassGauss(m1, PBH_mu, PBH_sigmam, PBH_norm)*PBH_MassGauss(m2, PBH_mu, PBH_sigmam, PBH_norm)\n",
    "    if PBH_LogNormal:\n",
    "        n = PBH_fR(z,f)*(year*GPc**3.)*DeVC(z, Delta_z)*(T_obs /(1. + z)) \\\n",
    "            *PBH_MassLNorm(m2, PBH_Mc, PBH_sigmamn, PBH_norm)*PBH_MassLNorm(m2, PBH_Mc, PBH_sigmamn, PBH_norm)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to estimate the analytical SGWB instead, we can define the integrated mass factor function as :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def MassFac_func(m1, m2, mu, sigma_m, Norm):\n",
    "        if PBH_Gaussian:\n",
    "            ris = PBH_MassGauss(m1, mu, sigma_m, Norm)*PBH_MassGauss(m2, mu, sigma_m, Norm)\\\n",
    "                   *((ChirpMass(m1,m2)*sol_mass)**(5./3.))\n",
    "        if PBH_LogNormal:\n",
    "            ris = PBH_MassLNorm(m1, mu, sigma_m, Norm)*PBH_MassLNorm(m2, mu, sigma_m, Norm)\\\n",
    "                   *((ChirpMass(m1,m2)*sol_mass)**(5./3.))\n",
    "        return ris                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PBH :\n",
    "    def PBH_AnalSGWB(i):\n",
    "        #Initializing instance variables\n",
    "        \n",
    "        z_fac = simpson(PBH_fRVec(SGWB_ranz,1.)*DtrDz(SGWB_ranz)/((1. + SGWB_ranz)**(1./3.)), SGWB_ranz)\n",
    "        \n",
    "        # Estimating the integral of the mass factor \n",
    "        \n",
    "        if PBH_Gaussian:\n",
    "            # Estimating the normalization constant for the Gaussian Case \n",
    "            PBH_GSnorm = PBH_GaussPS(SGWB_ranm1, 0.5*(PBH_pdfmspan[i + 1] + PBH_pdfmspan[i]), PBH_sigmam)\n",
    "            Part_MF = MassFac_func(SGWB_ranm1[None,: ], SGWB_ranm2, 0.5*(PBH_pdfmspan[i + 1] + PBH_pdfmspan[i]), PBH_sigmam, PBH_GSnorm)\n",
    "            mass_fac = simpson(simpson(Part_MF, x=SGWB_ranm2, axis=0), SGWB_ranm1 , axis=0)\n",
    "        \n",
    "        if PBH_LogNormal:\n",
    "            # Estimating the normalization constant for the LogNormal case\n",
    "            PBH_LNnorm = PBH_LNnormPS(SGWB_ranm1, 0.5*(PBH_pdfmspan[i + 1] + PBH_pdfmspan[i]), PBH_sigmamn)    \n",
    "            Part_MF = MassFac_func(SGWB_ranm1[None,: ], SGWB_ranm2, 0.5*(PBH_pdfmspan[i + 1] + PBH_pdfmspan[i]), PBH_sigmamn, PBH_LNnorm)\n",
    "            mass_fac = simpson(simpson(Part_MF, x=SGWB_ranm2, axis=0), SGWB_ranm1, axis=0)\n",
    "        \n",
    "        return [0.5*(PBH_pdfmspan[i + 1] + PBH_pdfmspan[i]),mass_fac*z_fac];\n",
    "                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Detector sensitivity curves </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of available detectors is given by:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Det_names = ['aplusLIGO', 'ET']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by implementing the analytical SNR on the [a+ LIGO](https://dcc.ligo.org/public/0149/T1800042/004/T1800042-v4.pdf) configuration, the detector frequency range for this configuration is given by :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIGO_f = 5. #Hz\n",
    "LIGO_F = 5000. #Hz\n",
    "LIGO_fprec = 5000\n",
    "\n",
    "LIGO_fran = np.linspace(LIGO_f,LIGO_F, LIGO_fprec)\n",
    "LIGO_mfran = 0.5*(LIGO_fran[1::] + LIGO_fran[:-1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and its sensitivity curve can be loaded from the following .txt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aplusLIGO_Sens = pd.read_csv('AplusDesign.txt', sep = \"  \", engine = 'python')\n",
    "LIGO_Sh = interp1d(aplusLIGO_Sens.Frequency, (aplusLIGO_Sens.aplusLIGO_Sh**2.), fill_value=\"extrapolate\")\n",
    "\n",
    "LIGO_cmi = LIGO_mfran*0.\n",
    "\n",
    "# Estimating the integrated factor of the analytical SNR estimator in order to fit an interpolator\n",
    "\n",
    "for i in range(len(LIGO_fran) - 1):\n",
    "            \n",
    "    if (i == 0):\n",
    "        LIGO_cmi[i] = (LIGO_fran[i + 1] - LIGO_fran[i])*((LIGO_mfran[i]**(-7./3.))/(LIGO_Sh(LIGO_mfran[i])))\n",
    "    else:\n",
    "        LIGO_cmi[i] = (LIGO_fran[i + 1] - LIGO_fran[i])*((LIGO_mfran[i]**(-7./3.))/(LIGO_Sh(LIGO_mfran[i])))\\\n",
    "        + LIGO_cmi[i - 1]\n",
    "\n",
    "aplusAnSNR_IntFac = interp1d(LIGO_mfran, LIGO_cmi, fill_value=\"extrapolate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can now implement the analytical SNR on the [ET](https://www.et-gw.eu/index.php) configuration, the detector frequency range in this case is given by:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ET_f = 0.1 #Hz\n",
    "ET_F = 10000. #Hz\n",
    "ET_fprec = 5000\n",
    "\n",
    "ET_fran = np.linspace(LIGO_f,LIGO_F, LIGO_fprec)\n",
    "ET_mfran = 0.5*(ET_fran[1::] + ET_fran[:-1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while its sensitivity curve can be loaded from the following .txt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ET_Sens = pd.read_csv('ETSens.txt', sep = \"   \", engine = 'python')\n",
    "ET_Sh = interp1d(ET_Sens.Frequency, (ET_Sens.ETSensD_Sum**2.), fill_value=\"extrapolate\")\n",
    "\n",
    "ET_cmi = ET_mfran*0.\n",
    "\n",
    "# Estimating the integrated factor of the analytical SNR estimator in order to fit an interpolator\n",
    "\n",
    "for i in range(len(ET_fran) - 1):\n",
    "            \n",
    "    if (i == 0):\n",
    "        ET_cmi[i] = (ET_fran[i + 1] - ET_fran[i])*((ET_mfran[i]**(-7./3.))/(ET_Sh(ET_mfran[i])))\n",
    "    else:\n",
    "        ET_cmi[i] = (ET_fran[i + 1] - ET_fran[i])*((ET_mfran[i]**(-7./3.))/(ET_Sh(ET_mfran[i])))\\\n",
    "        + ET_cmi[i - 1]\n",
    "\n",
    "ETAnSNR_IntFac = interp1d(ET_mfran, ET_cmi, fill_value=\"extrapolate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Analytcal SNR estimator </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand the number of resolvable events for each population, we are now gonna implement an analytical SNR approximator as presented in [S. Babak et al.](https://arxiv.org/abs/2108.01167). The frequency of coalescence can be approximated by using the $f_{ISCO}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetFisco(m1,m2):\n",
    "    M = m1 + m2 # Masses need to be in the source frame !\n",
    "    freq = (1./(6.*np.sqrt(6)*np.pi))*((c*1000)**3.)/(G*M*sol_mass) # Taken from eq 4.39 Maggiore\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fasten the code, we will choose two different approximations for the inclination. Depending on the choosen mode, the inclination of the events will be used differently in the waveform approximation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Inc_mode == 'max_i':\n",
    "    inc_fac = 8. # Signal maximized using best inclination\n",
    "if Inc_mode == 'avg_i':\n",
    "    inc_fac = 16./5. # Signal averaged over the possible inclinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally define the following function to estimate the analytical SNR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the waveform used for the SNR calculation\n",
    "def Stas_WF(f, m1, m2, DL):\n",
    "    Ch_M = ChirpMass(m1, m2)\n",
    "    res = (2./(np.pi)**(2./3.))*np.sqrt(5./96.)*(((sol_mass*Ch_M*G)**(5./6.))/(DL*MPc))*(1./(c*1000)**(3./2.))*(f**(-7./6.))\n",
    "    return res\n",
    "\n",
    "# Estimating the analytical SNR\n",
    "\n",
    "def AnalSNR(z, m1, m2, detector):\n",
    "    #Redshifting the masses\n",
    "    m1 = m1*(1. + z)\n",
    "    m2 = m2*(1. + z)\n",
    "    ChMass = ChirpMass(m1,m2)\n",
    "    Dl = (1. + z)*dist_func(z)\n",
    "    if detector == 'aplusLIGO':\n",
    "        LIGO_fend = min(GetFisco(m1, m2), LIGO_F)\n",
    "        res = np.sqrt(inc_fac*(aplusAnSNR_IntFac(LIGO_fend) - aplusAnSNR_IntFac(LIGO_f))*(((np.sqrt(5./96.)*((ChMass*sol_mass*G)**(5./6.)))/(Dl*GPc*np.pi**(2./3.)))**2.)*(1./(c*1000)**3.))\n",
    "    if detector == 'ET':\n",
    "        ET_fend = min(GetFisco(m1, m2), ET_F)\n",
    "        res = np.sqrt(inc_fac*(ETAnSNR_IntFac(ET_fend) - ETAnSNR_IntFac(ET_f))*(((np.sqrt(5./96.)*((ChMass*sol_mass*G)**(5./6.)))/(Dl*GPc*np.pi**(2./3.)))**2.)*(1./(c*1000)**3.))\n",
    "    \n",
    "    return res "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alternatively, we can also estimate the analytical SNR using the PyCBC waveforms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IMRPhenomD_AnalSNR(z, m1, m2):\n",
    "    # Return an array composed of [SNR_aplusLIGO, SNR_ET]\n",
    "    Dl = (1. + z)*dist_func(z)*(1.e3)\n",
    "    \n",
    "    # Creating the waveform using pycbc\n",
    "    \n",
    "    wave = wf.get_fd_waveform(approximant = 'IMRPhenomD', mass1 = m1*(1. + z), mass2 = m2*(1. + z), distance = Dl, delta_f = 0.1, f_lower = 2.5) # Mass need to be redshifted, distance in megaparsec\n",
    "    frq_span = wave[0].get_sample_frequencies() # 0 is for the + waveform, 1 is for x\n",
    "    \n",
    "    # Getting the mid point of the frequency and waveform array to integrate using trapeze method\n",
    "    \n",
    "    mid_frq = np.array(0.5*(frq_span[1::] + frq_span[:-1:]))\n",
    "    mid_wave = np.array(0.5*(abs(wave[0])[1::] + abs(wave[0])[:-1:]))\n",
    "    df = np.array(frq_span[1::] - frq_span[:-1:])\n",
    "    \n",
    "    # Cutting the waveform for frequency not in LIGO\n",
    "    \n",
    "    if min(mid_frq) < LIGO_f:\n",
    "        good_idx = mid_frq > LIGO_f\n",
    "        LIGO_frq = mid_frq[good_idx]\n",
    "        LIGO_df = df[good_idx]\n",
    "        LIGO_wave = mid_wave[good_idx]\n",
    "    \n",
    "    if max(LIGO_frq) > LIGO_F:\n",
    "        good_idx = LIGO_frq < LIGO_F\n",
    "        LIGO_frq = LIGO_frq[good_idx]\n",
    "        LIGO_df = LIGO_df[good_idx]\n",
    "        LIGO_wave = LIGO_wave[good_idx]\n",
    "        \n",
    "    # Cutting the waveform for frequency not in LIGO\n",
    "    \n",
    "    if min(mid_frq) < ET_f:\n",
    "        good_idx = mid_frq > ET_f\n",
    "        ET_frq = mid_frq[good_idx]\n",
    "        ET_df = df[good_idx]\n",
    "        ET_wave = mid_wave[good_idx]\n",
    "    \n",
    "    if max(ET_frq) > ET_F:\n",
    "        good_idx = ET_frq < ET_F\n",
    "        ET_frq = ET_frq[good_idx]\n",
    "        ET_df = ET_df[good_idx]\n",
    "        ET_wave = ET_wave[good_idx]\n",
    "    \n",
    "    #Now estimating the SNRs \n",
    "       \n",
    "    aplusLIGO_SNR = 4.*simpson((LIGO_wave**2.)/LIGO_Sh(LIGO_frq), LIGO_frq)\n",
    "    ET_SNR = 4.*simpson((ET_wave**2.)/ET_Sh(ET_frq), ET_frq)\n",
    "    \n",
    "    return [np.sqrt(aplusLIGO_SNR), np.sqrt(ET_SNR)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Setting of the analyzed phase space </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulation will be spanned over the following range of variables :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizialization of the frequency range and spectrum\n",
    "\n",
    "ran_frq = np.linspace(frq_min, frq_max, frq_prec)\n",
    "sensitivity = get_SciRD(ran_frq)\n",
    "spectrum = ran_frq * 0.\n",
    "t_0 = Z_to_HubbleTime(1.e-12) # Can't put 0 as the logarithmic scale would fail\n",
    "\n",
    "# Definition of the fiducial level of the SGWB and the various n-sigma values at the frequency of f_star in function of the noise level\n",
    "\n",
    "SGWB_FidNoise = [\n",
    "    1.8653859774892988e-12, # Interpolated from the analytical SGWB at frequency equal to f_star\n",
    "    1.9210318549184913e-12, # Obtained by the 1-sigma confidence ellipses with respect to the fiducial noise level\n",
    "    1.95702878750183e-12, # Obtained by the 2-sigma confidence ellipses with respect to the fiducial noise level\n",
    "    1.9937002425570923e-12 # Obtained by the 3-sigma confidence ellipses with respect to the fiducial noise level\n",
    "]\n",
    "\n",
    "# Initialization of the redshift phase space\n",
    "\n",
    "PBH_ranz = np.logspace(np.log10(PBH_zmin), np.log10(PBH_zmax), PBH_zprec)\n",
    "ran_d = Z_to_Gpc(PBH_ranz)\n",
    "dist_func = interp1d(PBH_ranz, ran_d, fill_value=\"extrapolate\")\n",
    "ran_d = 0.\n",
    "\n",
    "SGWB_ranz = np.logspace(np.log10(SGWB_zmin), np.log10(SGWB_zmax), SGWB_zprec)\n",
    "\n",
    "# Initialization of the SOBBH phase space\n",
    "    \n",
    "# Mass phase space\n",
    "    \n",
    "SOBBH_ranm1 = np.logspace(np.log10(SOBBH_m),np.log10(SOBBH_m + 5. - (SOBBH_M - (SOBBH_m + 5))/SOBBH_massprec), int(SOBBH_massprec/10))\n",
    "SOBBH_ranm1 = np.append(SOBBH_ranm1, np.linspace(SOBBH_m + 5., SOBBH_M,SOBBH_massprec))\n",
    "SOBBH_ranm2 = SOBBH_ranm1\n",
    "SOBBH_PLnorm = SOBBH_PowerLawPS(SOBBH_ranm1, SOBBH_m, SOBBH_M, SOBBH_alpha)\n",
    "SOBBH_GSnorm = SOBBH_GaussPS(SOBBH_ranm1, SOBBH_m, SOBBH_M, SOBBH_mum, SOBBH_sigmam) \n",
    "SOBBH_qnorm = SOBBH_P2PS(SOBBH_ranm2, SOBBH_betaq, SOBBH_m, SOBBH_deltam)\n",
    "SOBBH_MassPS = SOBBH_ModCPS(SOBBH_ranm1, SOBBH_ranm2, SOBBH_m, SOBBH_M, SOBBH_alpha, SOBBH_betaq, SOBBH_deltam, SOBBH_lambdapeak, SOBBH_mum, SOBBH_sigmam, SOBBH_PLnorm, SOBBH_GSnorm, SOBBH_qnorm, 1.)\n",
    "\n",
    "\n",
    "if SOBBH:\n",
    "    SOBBH_ranz = np.logspace(np.log10(SOBBH_z), np.log10(SOBBH_Zlog), SOBBH_zprec*2)\n",
    "    SOBBH_ranz = np.append(SOBBH_ranz, np.linspace(SOBBH_Zlog + (SOBBH_ranz[(SOBBH_zprec*2) - 1] - SOBBH_ranz[(SOBBH_zprec*2) - 2]), SOBBH_Zlin, SOBBH_zprec*100))\n",
    "    SOBBH_ranz = np.sort(SOBBH_ranz, kind = 'mergesort')\n",
    "# Distance phase space \n",
    "    \n",
    "if SOBBH_RSpike:\n",
    "    SOBBH_ranz = np.linspace(SOBBH_Rzmin, SOBBH_Rzmax, SOBBH_zprec)\n",
    "    SOBBH_ranampl = np.linspace(SOBBH_SpikeAmplMin, SOBBH_SpikeAmplMax, SOBBH_SpikeAmplPrec)\n",
    "\n",
    "    \n",
    "\n",
    "# Initialization of the PBH phase space        \n",
    "\n",
    "if PBH:\n",
    "    \n",
    "    # Mass phase space\n",
    "    \n",
    "    PBH_ranm1 = np.linspace(PBH_m, PBH_M, PBH_massprec)\n",
    "    PBH_ranm2 = PBH_ranm1\n",
    "    SGWB_ranm1 = 0.5*(PBH_ranm1[1::] + PBH_ranm1[:-1:])\n",
    "    SGWB_ranm2 = np.linspace(PBH_m, SGWB_ranm1, PBH_massprec)\n",
    "    PBH_frange = np.linspace(0.,1.,200)\n",
    "    q_norm = SOBBH_P2PS(PBH_ranm2, SOBBH_betaq, SOBBH_m, SOBBH_deltam)\n",
    "    \n",
    "    if PBH_fRt:\n",
    "        t_span = Z_to_HubbleTime(SGWB_ranz)\n",
    "        t_z = interpolate.interp1d(SGWB_ranz, t_span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Main body of the simulation </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may finally launch the pipeline to generate SGWB spectrum on every frequency bin of the frequency range, as well as the resolvable sources in function of $z$ both for the fiducial and subpopulation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by estimating the analytical SNR in each bin of the phase space by running the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the SNR matrix and defining their names\n",
    "\n",
    "LIGO_SNRs = np.zeros((len(PBH_ranz) - 1 ,len(PBH_ranm1) - 1, len(PBH_ranm2) - 1))\n",
    "ET_SNRs = np.zeros((len(PBH_ranz) - 1 ,len(PBH_ranm1) - 1, len(PBH_ranm2) - 1))\n",
    "LIGOSNRMat_filenm = Det_names[0]+'SNRMatZ'+str(PBH_zprec)+'M'+str(PBH_massprec)+'.pkl'\n",
    "ETSNRMat_filenm = Det_names[1]+'SNRMatZ'+str(PBH_zprec)+'M'+str(PBH_massprec)+'.pkl'\n",
    "\n",
    "if PBH and Compute_SNRMat:\n",
    "    manager = Manager()\n",
    "    print('-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~')\n",
    "    d_par = manager.list()\n",
    "    print('We are now estimating the SNR matrix over the parameter space')\n",
    "    \n",
    "    if __name__ == '__main__':                                    \n",
    "        # start the worker processes equals to n_jobs\n",
    "        pool = Pool(n_jobs)\n",
    "        pool.map(partial(AnalSNR_ParamSpace, mat = d_par), range(len(PBH_ranz)-1))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    \n",
    "    # Saving the SNR matrix over the phase space\n",
    "    \n",
    "    for count in range(len(d_par)):\n",
    "        LIGO_SNRs[int(d_par[count]['idx_z'])][int(d_par[count]['idx_m1'])][int(d_par[count]['idx_m2'])] = float(d_par[count]['aplusLIGO_val'])\n",
    "        ET_SNRs[int(d_par[count]['idx_z'])][int(d_par[count]['idx_m1'])][int(d_par[count]['idx_m2'])] = float(d_par[count]['ET_val'])\n",
    "\n",
    "    \n",
    "    with open(LIGOSNRMat_filenm,'wb') as file:\n",
    "        pickle.dump(LIGO_SNRs, file)\n",
    "    with open(ETSNRMat_filenm,'wb') as file:\n",
    "        pickle.dump(ET_SNRs, file)\n",
    "    print('-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~')\n",
    "\n",
    "\n",
    "else:\n",
    "    with open(LIGOSNRMat_filenm,'rb') as file:\n",
    "          LIGO_SNRs = pickle.load(file)\n",
    "    with open(ETSNRMat_filenm,'rb') as file:\n",
    "          ET_SNRs = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are analyzing a _PBH_ perturbation, the integrated factor in function of the _Mass PDF_ parameter can be estimated as :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if PBH and PBH_LogNormal:\n",
    "    \n",
    "    #Summing on the PBH background contribution in the case of a LogNormal PDF\n",
    "    print('-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~')\n",
    "    \n",
    "    manager = Manager()\n",
    "    LIGO_Zdom = {}\n",
    "    ET_Zdom = {}\n",
    "    d_ris = {}\n",
    "    d_rist = {}\n",
    "    \n",
    "    for i in range(len(PBH_sigmamnspan)):\n",
    "        print('-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~')\n",
    "        PBH_sigmamn = PBH_sigmamnspan[i]\n",
    "        print('Now simulating the Integrated factor for PBH (part ',i + 1,' of ',len(PBH_sigmamnspan),'), this can take some time !')\n",
    "        d_par = manager.list()\n",
    "        LIGO_app = np.zeros((len(PBH_pdfmspan) - 1, len(PBH_frange))) + 10.\n",
    "        ET_app = np.zeros((len(PBH_pdfmspan) - 1, len(PBH_frange))) + 10.\n",
    "        \n",
    "        print('First simulating the background levels for the various sub-populations at the given sigma')\n",
    "        \n",
    "        if __name__ == '__main__':                                    \n",
    "            # start the worker processes equals to n_jobs\n",
    "            pool = Pool(n_jobs)\n",
    "            d_ris[i] = pool.map(PBH_AnalSGWB, range(len(PBH_pdfmspan)-1))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "        \n",
    "        d_rist[i] = np.transpose(d_ris[i])\n",
    "            \n",
    "        print('Now estimating at which z the subpopulation will dominate in resolvable sources over the fiducial')\n",
    "        \n",
    "        if __name__ == '__main__':                                    \n",
    "            # start the worker processes equals to n_jobs\n",
    "            pool = Pool(n_jobs)\n",
    "            pool.map(partial(PBHvsFid_ResSrc, mat = d_par), range(len(PBH_pdfmspan)-1))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "        \n",
    "        for count in range(len(d_par)):\n",
    "            if Det_names[int(d_par[count]['Detector'])] == 'aplusLIGO':\n",
    "                LIGO_app[int(d_par[count]['idx_pdfm'])][int(d_par[count]['idx_Rf'])] = float(d_par[count]['v'])\n",
    "            if Det_names[int(d_par[count]['Detector'])] == 'ET':\n",
    "                ET_app[int(d_par[count]['idx_pdfm'])][int(d_par[count]['idx_Rf'])] = float(d_par[count]['v'])\n",
    "        \n",
    "        LIGO_Zdom[i] = LIGO_app.transpose()\n",
    "        ET_Zdom[i] = ET_app.transpose()\n",
    "        print('-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~')\n",
    "\n",
    "if PBH and PBH_Gaussian:\n",
    "    \n",
    "    #Summing on the PBH background contribution for the case of a Gaussian PDF with several sigma_m\n",
    "    print('-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~')\n",
    "    \n",
    "    manager = Manager()\n",
    "    LIGO_Zdom = {}\n",
    "    ET_Zdom = {}\n",
    "    d_ris = {}\n",
    "    d_rist = {}\n",
    "    \n",
    "    for i in range(len(PBH_sigmamspan)):\n",
    "        print('-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~')\n",
    "        PBH_sigmam = PBH_sigmamspan[i]\n",
    "        print('Now simulating the Integrated factor for PBH (part ',i + 1,' of ',len(PBH_sigmamspan),'), this can take some time !')\n",
    "        d_par = manager.list()\n",
    "        LIGO_app = np.zeros((len(PBH_pdfmspan) - 1, len(PBH_frange))) + 10.\n",
    "        ET_app = np.zeros((len(PBH_pdfmspan) - 1, len(PBH_frange))) + 10.\n",
    "        \n",
    "        print('First simulating the background levels for the various sub-populations at the given sigma')\n",
    "        \n",
    "        if __name__ == '__main__':                                    \n",
    "            # start the worker processes equals to n_jobs\n",
    "            pool = Pool(n_jobs)\n",
    "            d_ris[i] = pool.map(PBH_AnalSGWB, range(len(PBH_pdfmspan)-1))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "        \n",
    "        d_rist[i] = np.transpose(d_ris[i])\n",
    "        print('Now estimating at which z the subpopulation will dominate in resolvable sources over the fiducial')\n",
    "            \n",
    "        if __name__ == '__main__':                                    \n",
    "            # start the worker processes equals to n_jobs\n",
    "            pool = Pool(n_jobs)\n",
    "            pool.map(partial(PBHvsFid_ResSrc, mat = d_par), range(len(PBH_pdfmspan)-1))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "            \n",
    "        \n",
    "        for count in range(len(d_par)):\n",
    "            if Det_names[int(d_par[count]['Detector'])] == 'aplusLIGO':\n",
    "                LIGO_app[int(d_par[count]['idx_pdfm'])][int(d_par[count]['idx_Rf'])] = float(d_par[count]['v'])\n",
    "            if Det_names[int(d_par[count]['Detector'])] == 'ET':\n",
    "                ET_app[int(d_par[count]['idx_pdfm'])][int(d_par[count]['idx_Rf'])] = float(d_par[count]['v'])\n",
    "        \n",
    "        LIGO_Zdom[i] = LIGO_app.transpose()\n",
    "        ET_Zdom[i] = ET_app.transpose()\n",
    "        print('-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the resulting dataset, can be reordered as :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PBH and PBH_LogNormal:\n",
    "    data = {}\n",
    "    for i in range(len(PBH_sigmamnspan)):\n",
    "        data[i] = {'PDF_p1' : d_rist[i][0], 'IntFac' : d_rist[i][1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PBH and PBH_Gaussian:\n",
    "    data = {}\n",
    "    for i in range(len(PBH_sigmamspan)):\n",
    "        data[i] = {'PDF_p1' : d_rist[i][0], 'IntFac' : d_rist[i][1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Estimating the figure of merit grid values </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values estimated on the previous subsection, now need to be spanned over the phase space in order to plot the figure of merits in function of the parameters. Let's start by initializing the grid, for the PBH subpopulations we have: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PBH and PBH_LogNormal:\n",
    "\n",
    "    X = {}\n",
    "    Y = {}\n",
    "    Pert_PS = {}\n",
    "    App = {}\n",
    "    \n",
    "    for i in range(len(PBH_sigmamnspan)):\n",
    "        PBH_IntFacDP = pd.DataFrame(data[i])\n",
    "        PBH_IntFacDP = PBH_IntFacDP.sort_values([\"PDF_p1\", \"IntFac\"], ascending=True)\n",
    "        App[i] = PBH_IntFacDP['IntFac']\n",
    "        X[i], Y[i] = np.meshgrid(PBH_IntFacDP.PDF_p1, PBH_frange)\n",
    "        Pert_PS[i] = np.zeros((len(PBH_frange),len(PBH_IntFacDP.PDF_p1)))\n",
    "    \n",
    "if PBH and PBH_Gaussian:\n",
    "    \n",
    "    X = {}\n",
    "    Y = {}\n",
    "    Pert_PS = {}\n",
    "    App = {}\n",
    "    \n",
    "    for i in range(len(PBH_sigmamspan)):\n",
    "        PBH_IntFacDP = pd.DataFrame(data[i])\n",
    "        PBH_IntFacDP = PBH_IntFacDP.sort_values([\"PDF_p1\", \"IntFac\"], ascending=True)\n",
    "        App[i] = PBH_IntFacDP['IntFac']\n",
    "        X[i], Y[i] = np.meshgrid(PBH_IntFacDP.PDF_p1, PBH_frange)\n",
    "        Pert_PS[i] = np.zeros((len(PBH_frange),len(PBH_IntFacDP.PDF_p1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the two different cases, we can now fill the values of the grid as :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PBH and PBH_LogNormal:\n",
    "    for k in range(len(PBH_sigmamnspan)):\n",
    "        for i in range(len(PBH_IntFacDP.PDF_p1)):\n",
    "            for j in range(len(PBH_frange)):\n",
    "                Pert_PS[k][j][i] = (hc_to_Omega(1e-2,SOBBH_hcsqrd(1e-2, PBH_frange[j]*App[k][i])) + SGWB_FidNoise[0])\n",
    "\n",
    "if PBH and PBH_Gaussian:\n",
    "    for k in range(len(PBH_sigmamspan)):\n",
    "        for i in range(len(PBH_IntFacDP.PDF_p1)):\n",
    "            for j in range(len(PBH_frange)):\n",
    "                Pert_PS[k][j][i] = (hc_to_Omega(1e-2,SOBBH_hcsqrd(1e-2, PBH_frange[j]*App[k][i])) + SGWB_FidNoise[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving the dataset </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the data using :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the integrated factors for the SGWB Perturbation in each bin of parameter space\n",
    "\n",
    "if PBH and PBH_LogNormal:\n",
    "    fname = 'IntFacLNPDF.pickle'\n",
    "if PBH and PBH_Gaussian:\n",
    "    fname = 'IntFacGSPDF.pickle'\n",
    "\n",
    "if PBH_fRt :\n",
    "    fname = 'Rt' + fname\n",
    "        \n",
    "if PBH_fRz :\n",
    "    fname = 'Rz' + fname\n",
    "    \n",
    "file_to_write = open(fname, \"wb\")\n",
    "pickle.dump(Pert_PS, file_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the values of z at which the perturbation will overtake the fiducial models\n",
    "\n",
    "if PBH and PBH_LogNormal:\n",
    "    fname = 'RedDomLNPDF.pickle'\n",
    "if PBH and PBH_Gaussian:\n",
    "    fname = 'RedDomGSPDF.pickle'\n",
    "\n",
    "if PBH_fRt :\n",
    "    fname = 'aplusLIGO_Rt' + fname\n",
    "        \n",
    "if PBH_fRz :\n",
    "    fname = 'aplusLIGO_Rz' + fname\n",
    "    \n",
    "file_to_write = open(fname, \"wb\")\n",
    "pickle.dump(LIGO_Zdom, file_to_write)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the values of z at which the perturbation will overtake the fiducial models\n",
    "\n",
    "if PBH and PBH_LogNormal:\n",
    "    fname = 'RedDomLNPDF.pickle'\n",
    "if PBH and PBH_Gaussian:\n",
    "    fname = 'RedDomGSPDF.pickle'\n",
    "\n",
    "if PBH_fRt :\n",
    "    fname = 'ET_Rt' + fname\n",
    "        \n",
    "if PBH_fRz :\n",
    "    fname = 'ET_Rz' + fname\n",
    "    \n",
    "file_to_write = open(fname, \"wb\")\n",
    "pickle.dump(ET_Zdom, file_to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Setting alarm to inform when simulation is over </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = 'Alarm-ringtone.mp3'\n",
    "#os.system(\"mpg123 \"+file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
